{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>song</th>\n",
       "      <th>link</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Ahe's My Kind Of Girl</td>\n",
       "      <td>/a/abba/ahes+my+kind+of+girl_20598417.html</td>\n",
       "      <td>Look at her face, it's a wonderful face  \\nAnd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Andante, Andante</td>\n",
       "      <td>/a/abba/andante+andante_20002708.html</td>\n",
       "      <td>Take it easy with me, please  \\nTouch me gentl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>As Good As New</td>\n",
       "      <td>/a/abba/as+good+as+new_20003033.html</td>\n",
       "      <td>I'll never know why I had to go  \\nWhy I had t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Bang</td>\n",
       "      <td>/a/abba/bang_20598415.html</td>\n",
       "      <td>Making somebody happy is a question of give an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Bang-A-Boomerang</td>\n",
       "      <td>/a/abba/bang+a+boomerang_20002668.html</td>\n",
       "      <td>Making somebody happy is a question of give an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  artist                   song                                        link  \\\n",
       "0   ABBA  Ahe's My Kind Of Girl  /a/abba/ahes+my+kind+of+girl_20598417.html   \n",
       "1   ABBA       Andante, Andante       /a/abba/andante+andante_20002708.html   \n",
       "2   ABBA         As Good As New        /a/abba/as+good+as+new_20003033.html   \n",
       "3   ABBA                   Bang                  /a/abba/bang_20598415.html   \n",
       "4   ABBA       Bang-A-Boomerang      /a/abba/bang+a+boomerang_20002668.html   \n",
       "\n",
       "                                                text  \n",
       "0  Look at her face, it's a wonderful face  \\nAnd...  \n",
       "1  Take it easy with me, please  \\nTouch me gentl...  \n",
       "2  I'll never know why I had to go  \\nWhy I had t...  \n",
       "3  Making somebody happy is a question of give an...  \n",
       "4  Making somebody happy is a question of give an...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"data/songdata.zip\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded from file\n",
      "Vocab size: 76\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import util\n",
    "\n",
    "path = Path(\"chars.pkl\")\n",
    "chars = list()\n",
    "if path.is_file():\n",
    "    chars = util.load_vocab(path)\n",
    "    print(\"Loaded from file\")\n",
    "else:\n",
    "    vocab = set()\n",
    "    for song in df[\"text\"]:\n",
    "        chars = set(song)\n",
    "        vocab = vocab.union(chars)\n",
    "    chars = list(vocab)\n",
    "    util.write_vocab(path, chars)\n",
    "    print(\"Generated from source\")\n",
    "    \n",
    "vocab_size = len(chars)\n",
    "print(\"Vocab size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "char2idx = { char:i for i,char in enumerate(chars) }\n",
    "idx2char = { i:char for i,char in enumerate(chars) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_samples(song, buffer_length):\n",
    "    tokens = song\n",
    "\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    for i in range(0, len(song)):\n",
    "        if i+buffer_length+1 >= len(tokens):\n",
    "            continue\n",
    "            \n",
    "        x_train.append(tokens[i:i+buffer_length])\n",
    "        y_train.append(tokens[i+buffer_length])\n",
    "\n",
    "    return x_train,y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data length: 581323\n",
      "X[0]: Look a\n",
      "Y[0]: t\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train = [], []\n",
    "for song in df[\"text\"][:500]:\n",
    "    xs, ys = build_samples(song, 6)\n",
    "    x_train.extend(xs)\n",
    "    y_train.extend(ys)\n",
    "print(\"Training data length:\", len(x_train))\n",
    "print(\"X[0]:\", x_train[0])\n",
    "print(\"Y[0]:\", y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "\n",
    "# 80% Train, 10% Dev, 10% Test\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(x_train, y_train, test_size=0.2)\n",
    "X_dev, X_test, Y_dev, Y_test = train_test_split(X_test, Y_test, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import shuffle\n",
    "\n",
    "SEQUENCE_LENGTH = 6\n",
    "\n",
    "def load_batch(xs, ys, begin, end):\n",
    "    batch_size = end-begin\n",
    "    \n",
    "    x_train = np.zeros((batch_size, SEQUENCE_LENGTH, vocab_size))\n",
    "    y_train = np.zeros((batch_size, vocab_size))\n",
    "    \n",
    "    xs_batch = xs[begin:end]\n",
    "    ys_batch = ys[begin:end]\n",
    "    \n",
    "    c = list(zip(xs_batch, ys_batch))\n",
    "    shuffle(c)\n",
    "    xs_batch, ys_batch = zip(*c)\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        x_train[i] = util.one_hot_encode_sequence(xs_batch[i], char2idx)\n",
    "        y_train[i] = util.one_hot_encode(ys_batch[i], char2idx)\n",
    "    \n",
    "    return x_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 6, 76) (512, 76)\n"
     ]
    }
   ],
   "source": [
    "batches = util.generate_batches(len(X_train), 512)\n",
    "begin, end = next(batches)\n",
    "x_batch, y_batch = load_batch(X_train, Y_train, begin, end)\n",
    "print(x_batch.shape, y_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 'e's ju' -> s\n",
      "1 'see yo' -> u\n",
      "2 'e toni' -> g\n",
      "3 ' show)' ->  \n",
      "4 's  \n",
      "In' ->  \n",
      "5 'try to' ->  \n",
      "6 't  \n",
      "An' -> d\n",
      "7 't you ' -> l\n",
      "8 'ed you' ->  \n",
      "9 'ap wit' -> h\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    xs = ''.join(util.one_hot_decode_sequence(x_batch[i], idx2char))\n",
    "    y = util.one_hot_decode(y_batch[i], idx2char)\n",
    "    print(f\"{i} '{xs}' -> {y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded!\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_2 (LSTM)                (None, 512)               1206272   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 76)                38988     \n",
      "=================================================================\n",
      "Total params: 1,245,260\n",
      "Trainable params: 1,245,260\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Embedding, Dropout, GRU\n",
    "from keras.layers import LeakyReLU\n",
    "from pathlib import Path\n",
    "\n",
    "def build_model(vocab_size):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(512, input_shape=(SEQUENCE_LENGTH, vocab_size)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    model.add(Dense(vocab_size, activation='softmax'))\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer=\"adam\", metrics = ['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = build_model(vocab_size)\n",
    "weights_path = Path(\"weights_char_rnn.h5\")\n",
    "if weights_path.is_file():\n",
    "    print(\"Loaded!\")\n",
    "    model.load_weights(weights_path.resolve())\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches: 57\n"
     ]
    }
   ],
   "source": [
    "batch_size = 8192\n",
    "batches = generate_batches(len(X_train), batch_size)\n",
    "print(\"Number of batches:\", math.ceil(len(X_train) / batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "8192/8192 [==============================] - 1s 175us/step - loss: 1.7251 - accuracy: 0.5051\n",
      "Epoch 2/4\n",
      "8192/8192 [==============================] - 1s 180us/step - loss: 1.6197 - accuracy: 0.5283\n",
      "Epoch 3/4\n",
      "8192/8192 [==============================] - 1s 182us/step - loss: 1.5395 - accuracy: 0.5463\n",
      "Epoch 4/4\n",
      "8192/8192 [==============================] - 2s 184us/step - loss: 1.4483 - accuracy: 0.5706\n",
      "Epoch 1/4\n",
      "8192/8192 [==============================] - 2s 184us/step - loss: 1.7270 - accuracy: 0.5105\n",
      "Epoch 2/4\n",
      "8192/8192 [==============================] - 2s 187us/step - loss: 1.6027 - accuracy: 0.5378\n",
      "Epoch 3/4\n",
      "8192/8192 [==============================] - 1s 182us/step - loss: 1.4977 - accuracy: 0.5603\n",
      "Epoch 4/4\n",
      "8192/8192 [==============================] - 2s 187us/step - loss: 1.4241 - accuracy: 0.5819\n",
      "Epoch 1/4\n",
      "8192/8192 [==============================] - 2s 190us/step - loss: 1.7623 - accuracy: 0.5022\n",
      "Epoch 2/4\n",
      "8192/8192 [==============================] - 2s 198us/step - loss: 1.6176 - accuracy: 0.5349\n",
      "Epoch 3/4\n",
      "8192/8192 [==============================] - 2s 214us/step - loss: 1.5349 - accuracy: 0.5492\n",
      "Epoch 4/4\n",
      "8192/8192 [==============================] - 2s 204us/step - loss: 1.4516 - accuracy: 0.5717\n",
      "Epoch 1/4\n",
      "8192/8192 [==============================] - 2s 200us/step - loss: 1.6939 - accuracy: 0.5183\n",
      "Epoch 2/4\n",
      "8192/8192 [==============================] - 2s 203us/step - loss: 1.5607 - accuracy: 0.5425\n",
      "Epoch 3/4\n",
      "8192/8192 [==============================] - 2s 203us/step - loss: 1.4609 - accuracy: 0.5724\n",
      "Epoch 4/4\n",
      "8192/8192 [==============================] - 2s 202us/step - loss: 1.3821 - accuracy: 0.5894\n",
      "Epoch 1/4\n",
      "8192/8192 [==============================] - 2s 209us/step - loss: 1.7057 - accuracy: 0.5120\n",
      "Epoch 2/4\n",
      "8192/8192 [==============================] - 2s 192us/step - loss: 1.5451 - accuracy: 0.5533\n",
      "Epoch 3/4\n",
      "8192/8192 [==============================] - 2s 200us/step - loss: 1.4494 - accuracy: 0.5685\n",
      "Epoch 4/4\n",
      "8192/8192 [==============================] - 2s 196us/step - loss: 1.3869 - accuracy: 0.5881\n",
      "Epoch 1/4\n",
      "8192/8192 [==============================] - 2s 202us/step - loss: 1.7366 - accuracy: 0.5068\n",
      "Epoch 2/4\n",
      "8192/8192 [==============================] - 2s 198us/step - loss: 1.5935 - accuracy: 0.5377\n",
      "Epoch 3/4\n",
      "8192/8192 [==============================] - 2s 199us/step - loss: 1.5021 - accuracy: 0.5640\n",
      "Epoch 4/4\n",
      "8192/8192 [==============================] - 2s 204us/step - loss: 1.4189 - accuracy: 0.5865\n",
      "Epoch 1/4\n",
      "8192/8192 [==============================] - 2s 211us/step - loss: 1.7009 - accuracy: 0.5116\n",
      "Epoch 2/4\n",
      "8192/8192 [==============================] - 2s 198us/step - loss: 1.5485 - accuracy: 0.5511\n",
      "Epoch 3/4\n",
      "8192/8192 [==============================] - 2s 201us/step - loss: 1.4645 - accuracy: 0.5659\n",
      "Epoch 4/4\n",
      "8192/8192 [==============================] - 2s 206us/step - loss: 1.3707 - accuracy: 0.5874\n",
      "Epoch 1/4\n",
      "8192/8192 [==============================] - 2s 216us/step - loss: 1.6787 - accuracy: 0.5234\n",
      "Epoch 2/4\n",
      "8192/8192 [==============================] - 2s 203us/step - loss: 1.5293 - accuracy: 0.5558\n",
      "Epoch 3/4\n",
      "8192/8192 [==============================] - 2s 207us/step - loss: 1.4317 - accuracy: 0.5782\n",
      "Epoch 4/4\n",
      "8192/8192 [==============================] - 2s 205us/step - loss: 1.3485 - accuracy: 0.6008\n",
      "Epoch 1/4\n",
      "8192/8192 [==============================] - 2s 205us/step - loss: 1.6921 - accuracy: 0.5227\n",
      "Epoch 2/4\n",
      "8192/8192 [==============================] - 2s 212us/step - loss: 1.5421 - accuracy: 0.5573\n",
      "Epoch 3/4\n",
      "8192/8192 [==============================] - 2s 215us/step - loss: 1.4359 - accuracy: 0.5822\n",
      "Epoch 4/4\n",
      "8192/8192 [==============================] - 2s 208us/step - loss: 1.3565 - accuracy: 0.6023\n",
      "Epoch 1/4\n",
      "8192/8192 [==============================] - 2s 224us/step - loss: 1.6798 - accuracy: 0.5287\n",
      "Epoch 2/4\n",
      "8192/8192 [==============================] - 2s 206us/step - loss: 1.5428 - accuracy: 0.5554\n",
      "Epoch 3/4\n",
      "8192/8192 [==============================] - 2s 206us/step - loss: 1.4406 - accuracy: 0.5756\n",
      "Epoch 4/4\n",
      "8192/8192 [==============================] - 2s 203us/step - loss: 1.3616 - accuracy: 0.6000\n",
      "Epoch 1/4\n",
      "8192/8192 [==============================] - 2s 216us/step - loss: 1.6744 - accuracy: 0.5239\n",
      "Epoch 2/4\n",
      "8192/8192 [==============================] - 2s 221us/step - loss: 1.5178 - accuracy: 0.5551\n",
      "Epoch 3/4\n",
      "8192/8192 [==============================] - 2s 208us/step - loss: 1.4331 - accuracy: 0.5813\n",
      "Epoch 4/4\n",
      "8192/8192 [==============================] - 2s 224us/step - loss: 1.3355 - accuracy: 0.6057\n",
      "Epoch 1/4\n",
      "8192/8192 [==============================] - 2s 206us/step - loss: 1.6526 - accuracy: 0.5255\n",
      "Epoch 2/4\n",
      "8192/8192 [==============================] - 2s 218us/step - loss: 1.4944 - accuracy: 0.5653\n",
      "Epoch 3/4\n",
      "8192/8192 [==============================] - 2s 228us/step - loss: 1.4074 - accuracy: 0.5822\n",
      "Epoch 4/4\n",
      "8192/8192 [==============================] - 2s 224us/step - loss: 1.3251 - accuracy: 0.6047\n",
      "Epoch 1/4\n",
      "8192/8192 [==============================] - 2s 206us/step - loss: 1.6544 - accuracy: 0.5256\n",
      "Epoch 2/4\n",
      "8192/8192 [==============================] - 2s 210us/step - loss: 1.4829 - accuracy: 0.5645\n",
      "Epoch 3/4\n",
      "8192/8192 [==============================] - 2s 207us/step - loss: 1.3926 - accuracy: 0.5884\n",
      "Epoch 4/4\n",
      "8192/8192 [==============================] - 2s 207us/step - loss: 1.3020 - accuracy: 0.6146\n",
      "Epoch 1/4\n",
      "8192/8192 [==============================] - 2s 197us/step - loss: 1.6710 - accuracy: 0.5249\n",
      "Epoch 2/4\n",
      "8192/8192 [==============================] - 2s 195us/step - loss: 1.5177 - accuracy: 0.5602\n",
      "Epoch 3/4\n",
      "8192/8192 [==============================] - 2s 189us/step - loss: 1.4225 - accuracy: 0.5779\n",
      "Epoch 4/4\n",
      "8192/8192 [==============================] - 2s 191us/step - loss: 1.3321 - accuracy: 0.6051\n",
      "Epoch 1/4\n",
      "8192/8192 [==============================] - 2s 193us/step - loss: 1.6250 - accuracy: 0.5353\n",
      "Epoch 2/4\n",
      "8192/8192 [==============================] - 2s 191us/step - loss: 1.4760 - accuracy: 0.5691\n",
      "Epoch 3/4\n",
      "8192/8192 [==============================] - 2s 188us/step - loss: 1.3841 - accuracy: 0.5920\n",
      "Epoch 4/4\n",
      "8192/8192 [==============================] - 2s 190us/step - loss: 1.2917 - accuracy: 0.6207\n",
      "Epoch 1/4\n",
      "8192/8192 [==============================] - 2s 193us/step - loss: 1.6104 - accuracy: 0.5389\n",
      "Epoch 2/4\n",
      "8192/8192 [==============================] - 2s 194us/step - loss: 1.4728 - accuracy: 0.5704\n",
      "Epoch 3/4\n",
      "8192/8192 [==============================] - 2s 191us/step - loss: 1.3602 - accuracy: 0.5934\n",
      "Epoch 4/4\n",
      "8192/8192 [==============================] - 2s 194us/step - loss: 1.2888 - accuracy: 0.6188\n",
      "Epoch 1/4\n",
      "8192/8192 [==============================] - 2s 192us/step - loss: 1.6331 - accuracy: 0.5354\n",
      "Epoch 2/4\n",
      "8192/8192 [==============================] - 2s 203us/step - loss: 1.4685 - accuracy: 0.5687\n",
      "Epoch 3/4\n",
      "8192/8192 [==============================] - 2s 196us/step - loss: 1.3890 - accuracy: 0.5941\n",
      "Epoch 4/4\n",
      "8192/8192 [==============================] - 2s 197us/step - loss: 1.2904 - accuracy: 0.6113\n",
      "Epoch 1/4\n",
      "8192/8192 [==============================] - 2s 204us/step - loss: 1.6383 - accuracy: 0.5354\n",
      "Epoch 2/4\n",
      "8192/8192 [==============================] - 2s 212us/step - loss: 1.4811 - accuracy: 0.5704\n",
      "Epoch 3/4\n",
      "8192/8192 [==============================] - 2s 205us/step - loss: 1.3644 - accuracy: 0.5990\n",
      "Epoch 4/4\n",
      "8192/8192 [==============================] - 2s 197us/step - loss: 1.2749 - accuracy: 0.6227\n",
      "Epoch 1/4\n",
      "8192/8192 [==============================] - 2s 199us/step - loss: 1.6418 - accuracy: 0.5331\n",
      "Epoch 2/4\n",
      "8192/8192 [==============================] - 2s 191us/step - loss: 1.4759 - accuracy: 0.5690\n",
      "Epoch 3/4\n",
      "8192/8192 [==============================] - 2s 189us/step - loss: 1.3878 - accuracy: 0.5902\n",
      "Epoch 4/4\n",
      "8192/8192 [==============================] - 2s 195us/step - loss: 1.2960 - accuracy: 0.6176\n",
      "Epoch 1/4\n",
      "8192/8192 [==============================] - 2s 187us/step - loss: 1.6206 - accuracy: 0.5363\n",
      "Epoch 2/4\n",
      "8192/8192 [==============================] - 2s 194us/step - loss: 1.4578 - accuracy: 0.5756\n",
      "Epoch 3/4\n",
      "8192/8192 [==============================] - 2s 191us/step - loss: 1.3671 - accuracy: 0.5942\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8192/8192 [==============================] - 2s 185us/step - loss: 1.2835 - accuracy: 0.6218\n",
      "Epoch 1/4\n",
      "8192/8192 [==============================] - 1s 180us/step - loss: 1.6212 - accuracy: 0.5388\n",
      "Epoch 2/4\n",
      "8192/8192 [==============================] - 1s 183us/step - loss: 1.4662 - accuracy: 0.5741\n",
      "Epoch 3/4\n",
      "8192/8192 [==============================] - 2s 185us/step - loss: 1.3535 - accuracy: 0.5947\n",
      "Epoch 4/4\n",
      "8192/8192 [==============================] - 2s 188us/step - loss: 1.2686 - accuracy: 0.6206\n",
      "Epoch 1/4\n",
      "8192/8192 [==============================] - 1s 181us/step - loss: 1.5869 - accuracy: 0.5448\n",
      "Epoch 2/4\n",
      "8192/8192 [==============================] - 1s 183us/step - loss: 1.4381 - accuracy: 0.5792\n",
      "Epoch 3/4\n",
      "8192/8192 [==============================] - 2s 190us/step - loss: 1.3462 - accuracy: 0.6029\n",
      "Epoch 4/4\n",
      "8192/8192 [==============================] - 2s 191us/step - loss: 1.2450 - accuracy: 0.6257\n",
      "Epoch 1/4\n",
      "8192/8192 [==============================] - 1s 181us/step - loss: 1.6185 - accuracy: 0.5435\n",
      "Epoch 2/4\n",
      "8192/8192 [==============================] - 1s 180us/step - loss: 1.4586 - accuracy: 0.5708\n",
      "Epoch 3/4\n",
      "8192/8192 [==============================] - 1s 181us/step - loss: 1.3552 - accuracy: 0.5981\n",
      "Epoch 4/4\n",
      "8192/8192 [==============================] - 1s 182us/step - loss: 1.2793 - accuracy: 0.6193\n",
      "Epoch 1/4\n",
      "8192/8192 [==============================] - 2s 188us/step - loss: 1.5876 - accuracy: 0.5474\n",
      "Epoch 2/4\n",
      "8192/8192 [==============================] - 2s 185us/step - loss: 1.4356 - accuracy: 0.5796\n",
      "Epoch 3/4\n",
      "8192/8192 [==============================] - 2s 189us/step - loss: 1.3245 - accuracy: 0.6028\n",
      "Epoch 4/4\n",
      "8192/8192 [==============================] - 2s 190us/step - loss: 1.2369 - accuracy: 0.6250\n",
      "Epoch 1/4\n",
      "8192/8192 [==============================] - 2s 187us/step - loss: 1.6283 - accuracy: 0.5345\n",
      "Epoch 2/4\n",
      "8192/8192 [==============================] - 2s 191us/step - loss: 1.4637 - accuracy: 0.5682\n",
      "Epoch 3/4\n",
      "8192/8192 [==============================] - 2s 194us/step - loss: 1.3647 - accuracy: 0.5985\n",
      "Epoch 4/4\n",
      "8192/8192 [==============================] - 2s 192us/step - loss: 1.2598 - accuracy: 0.6240\n",
      "Epoch 1/4\n",
      "8192/8192 [==============================] - 2s 186us/step - loss: 1.6112 - accuracy: 0.5337\n",
      "Epoch 2/4\n",
      "8192/8192 [==============================] - 2s 196us/step - loss: 1.4431 - accuracy: 0.5765\n",
      "Epoch 3/4\n",
      "8192/8192 [==============================] - 2s 192us/step - loss: 1.3338 - accuracy: 0.6023\n",
      "Epoch 4/4\n",
      "8192/8192 [==============================] - 2s 188us/step - loss: 1.2570 - accuracy: 0.6239\n",
      "Epoch 1/4\n",
      "8192/8192 [==============================] - 2s 187us/step - loss: 1.6021 - accuracy: 0.5426\n",
      "Epoch 2/4\n",
      "8192/8192 [==============================] - 2s 185us/step - loss: 1.4285 - accuracy: 0.5809\n",
      "Epoch 3/4\n",
      "8192/8192 [==============================] - 2s 189us/step - loss: 1.3238 - accuracy: 0.6125\n",
      "Epoch 4/4\n",
      "8192/8192 [==============================] - 2s 190us/step - loss: 1.2403 - accuracy: 0.6305\n",
      "Epoch 1/4\n",
      "8192/8192 [==============================] - 2s 186us/step - loss: 1.6357 - accuracy: 0.5375\n",
      "Epoch 2/4\n",
      "8192/8192 [==============================] - 2s 186us/step - loss: 1.4805 - accuracy: 0.5671\n",
      "Epoch 3/4\n",
      "8192/8192 [==============================] - 2s 188us/step - loss: 1.3753 - accuracy: 0.5919\n",
      "Epoch 4/4\n",
      "8192/8192 [==============================] - 2s 188us/step - loss: 1.2706 - accuracy: 0.6277\n",
      "Epoch 1/4\n",
      "8192/8192 [==============================] - 1s 183us/step - loss: 1.6085 - accuracy: 0.5374\n",
      "Epoch 2/4\n",
      "8192/8192 [==============================] - 2s 194us/step - loss: 1.4479 - accuracy: 0.5753\n",
      "Epoch 3/4\n",
      "8192/8192 [==============================] - 2s 186us/step - loss: 1.3454 - accuracy: 0.6002\n",
      "Epoch 4/4\n",
      "8192/8192 [==============================] - 2s 187us/step - loss: 1.2415 - accuracy: 0.6255\n",
      "Epoch 1/4\n",
      "8192/8192 [==============================] - 2s 185us/step - loss: 1.6183 - accuracy: 0.5441\n",
      "Epoch 2/4\n",
      "8192/8192 [==============================] - 2s 195us/step - loss: 1.4341 - accuracy: 0.5862\n",
      "Epoch 3/4\n",
      "8192/8192 [==============================] - 2s 190us/step - loss: 1.3323 - accuracy: 0.6117\n",
      "Epoch 4/4\n",
      "8192/8192 [==============================] - 2s 195us/step - loss: 1.2416 - accuracy: 0.6355\n",
      "Epoch 1/4\n",
      "8192/8192 [==============================] - 2s 195us/step - loss: 1.5958 - accuracy: 0.5383\n",
      "Epoch 2/4\n",
      "8192/8192 [==============================] - 2s 192us/step - loss: 1.4194 - accuracy: 0.5803\n",
      "Epoch 3/4\n",
      "8192/8192 [==============================] - 2s 200us/step - loss: 1.3167 - accuracy: 0.6069\n",
      "Epoch 4/4\n",
      "8192/8192 [==============================] - 2s 186us/step - loss: 1.2389 - accuracy: 0.6249\n",
      "Epoch 1/4\n",
      "8192/8192 [==============================] - 2s 192us/step - loss: 1.5849 - accuracy: 0.5496\n",
      "Epoch 2/4\n",
      "8192/8192 [==============================] - 2s 197us/step - loss: 1.4126 - accuracy: 0.5852\n",
      "Epoch 3/4\n",
      "8192/8192 [==============================] - 2s 209us/step - loss: 1.2995 - accuracy: 0.6167\n",
      "Epoch 4/4\n",
      "8192/8192 [==============================] - 2s 200us/step - loss: 1.2160 - accuracy: 0.6379\n",
      "Epoch 1/4\n",
      "8192/8192 [==============================] - 2s 202us/step - loss: 1.5719 - accuracy: 0.5410\n",
      "Epoch 2/4\n",
      "8192/8192 [==============================] - 2s 207us/step - loss: 1.4185 - accuracy: 0.5836\n",
      "Epoch 3/4\n",
      "8192/8192 [==============================] - 2s 202us/step - loss: 1.2945 - accuracy: 0.6167\n",
      "Epoch 4/4\n",
      "8192/8192 [==============================] - 2s 201us/step - loss: 1.2203 - accuracy: 0.6334\n",
      "Epoch 1/4\n",
      "8192/8192 [==============================] - 2s 206us/step - loss: 1.5790 - accuracy: 0.5514\n",
      "Epoch 2/4\n",
      "8192/8192 [==============================] - 2s 196us/step - loss: 1.4163 - accuracy: 0.5869\n",
      "Epoch 3/4\n",
      "8192/8192 [==============================] - 2s 184us/step - loss: 1.2975 - accuracy: 0.6150\n",
      "Epoch 4/4\n",
      "8192/8192 [==============================] - 2s 189us/step - loss: 1.2134 - accuracy: 0.6394\n",
      "Epoch 1/4\n",
      "8192/8192 [==============================] - 2s 187us/step - loss: 1.5171 - accuracy: 0.5604\n",
      "Epoch 2/4\n",
      "8192/8192 [==============================] - 2s 193us/step - loss: 1.3441 - accuracy: 0.6045\n",
      "Epoch 3/4\n",
      "8192/8192 [==============================] - 2s 197us/step - loss: 1.2383 - accuracy: 0.6254\n",
      "Epoch 4/4\n",
      "8192/8192 [==============================] - 2s 210us/step - loss: 1.1555 - accuracy: 0.6509\n",
      "Epoch 1/4\n",
      "8192/8192 [==============================] - 2s 199us/step - loss: 1.5382 - accuracy: 0.5540\n",
      "Epoch 2/4\n",
      "8192/8192 [==============================] - 2s 202us/step - loss: 1.3796 - accuracy: 0.5957\n",
      "Epoch 3/4\n",
      "8192/8192 [==============================] - 2s 211us/step - loss: 1.2658 - accuracy: 0.6190\n",
      "Epoch 4/4\n",
      "8192/8192 [==============================] - 2s 205us/step - loss: 1.1764 - accuracy: 0.6440\n",
      "Epoch 1/4\n",
      "8192/8192 [==============================] - 2s 202us/step - loss: 1.5821 - accuracy: 0.5509\n",
      "Epoch 2/4\n",
      "8192/8192 [==============================] - 2s 203us/step - loss: 1.4056 - accuracy: 0.5820\n",
      "Epoch 3/4\n",
      "8192/8192 [==============================] - 2s 198us/step - loss: 1.2916 - accuracy: 0.6134\n",
      "Epoch 4/4\n",
      "8192/8192 [==============================] - 2s 187us/step - loss: 1.1926 - accuracy: 0.6411\n",
      "Epoch 1/4\n",
      "8192/8192 [==============================] - 2s 189us/step - loss: 1.5775 - accuracy: 0.5499\n",
      "Epoch 2/4\n",
      "8192/8192 [==============================] - 2s 192us/step - loss: 1.3997 - accuracy: 0.5880\n",
      "Epoch 3/4\n",
      "8192/8192 [==============================] - 2s 192us/step - loss: 1.2895 - accuracy: 0.6162\n",
      "Epoch 4/4\n",
      "8192/8192 [==============================] - 2s 206us/step - loss: 1.1917 - accuracy: 0.6440\n",
      "Epoch 1/4\n",
      "8192/8192 [==============================] - 2s 204us/step - loss: 1.5976 - accuracy: 0.5413\n",
      "Epoch 2/4\n",
      "8192/8192 [==============================] - 2s 204us/step - loss: 1.4147 - accuracy: 0.5806\n",
      "Epoch 3/4\n",
      "8192/8192 [==============================] - 2s 205us/step - loss: 1.3140 - accuracy: 0.6055\n",
      "Epoch 4/4\n",
      "8192/8192 [==============================] - 2s 206us/step - loss: 1.2116 - accuracy: 0.6324\n",
      "Epoch 1/4\n",
      "8192/8192 [==============================] - 2s 197us/step - loss: 1.5688 - accuracy: 0.5392\n",
      "Epoch 2/4\n",
      "8192/8192 [==============================] - 2s 203us/step - loss: 1.3855 - accuracy: 0.5847\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8192/8192 [==============================] - 2s 194us/step - loss: 1.2776 - accuracy: 0.6147\n",
      "Epoch 4/4\n",
      "8192/8192 [==============================] - 2s 184us/step - loss: 1.1931 - accuracy: 0.6387\n",
      "Epoch 1/4\n",
      "8192/8192 [==============================] - 1s 182us/step - loss: 1.5774 - accuracy: 0.5419\n",
      "Epoch 2/4\n",
      "8192/8192 [==============================] - 2s 184us/step - loss: 1.3949 - accuracy: 0.5930\n",
      "Epoch 3/4\n",
      "8192/8192 [==============================] - 2s 186us/step - loss: 1.2798 - accuracy: 0.6189\n",
      "Epoch 4/4\n",
      "8192/8192 [==============================] - 2s 197us/step - loss: 1.1998 - accuracy: 0.6421\n",
      "Epoch 1/4\n",
      "8192/8192 [==============================] - 2s 195us/step - loss: 1.5637 - accuracy: 0.5472\n",
      "Epoch 2/4\n",
      "8192/8192 [==============================] - 2s 188us/step - loss: 1.3963 - accuracy: 0.5885\n",
      "Epoch 3/4\n",
      "8192/8192 [==============================] - 2s 189us/step - loss: 1.2858 - accuracy: 0.6210\n",
      "Epoch 4/4\n",
      "8192/8192 [==============================] - 2s 199us/step - loss: 1.1948 - accuracy: 0.6389\n",
      "Epoch 1/4\n",
      "8192/8192 [==============================] - 2s 193us/step - loss: 1.5682 - accuracy: 0.5521\n",
      "Epoch 2/4\n",
      "8192/8192 [==============================] - 2s 198us/step - loss: 1.3920 - accuracy: 0.5916\n",
      "Epoch 3/4\n",
      "8192/8192 [==============================] - 2s 193us/step - loss: 1.2844 - accuracy: 0.6174\n",
      "Epoch 4/4\n",
      "8192/8192 [==============================] - 2s 192us/step - loss: 1.1973 - accuracy: 0.6384\n",
      "Epoch 1/4\n",
      "8192/8192 [==============================] - 2s 183us/step - loss: 1.5410 - accuracy: 0.5568\n",
      "Epoch 2/4\n",
      "8192/8192 [==============================] - 2s 188us/step - loss: 1.3748 - accuracy: 0.5917\n",
      "Epoch 3/4\n",
      "8192/8192 [==============================] - 1s 183us/step - loss: 1.2608 - accuracy: 0.6263\n",
      "Epoch 4/4\n",
      "8192/8192 [==============================] - 2s 194us/step - loss: 1.1742 - accuracy: 0.6454\n",
      "Epoch 1/4\n",
      "8192/8192 [==============================] - 2s 188us/step - loss: 1.5607 - accuracy: 0.5530\n",
      "Epoch 2/4\n",
      "8192/8192 [==============================] - 2s 188us/step - loss: 1.3827 - accuracy: 0.5955\n",
      "Epoch 3/4\n",
      "8192/8192 [==============================] - 2s 187us/step - loss: 1.2703 - accuracy: 0.6195\n",
      "Epoch 4/4\n",
      "8192/8192 [==============================] - 2s 195us/step - loss: 1.1818 - accuracy: 0.6445\n",
      "Epoch 1/4\n",
      "8192/8192 [==============================] - 1s 181us/step - loss: 1.5409 - accuracy: 0.5634\n",
      "Epoch 2/4\n",
      "8192/8192 [==============================] - 2s 188us/step - loss: 1.3685 - accuracy: 0.5969\n",
      "Epoch 3/4\n",
      "8192/8192 [==============================] - 2s 185us/step - loss: 1.2532 - accuracy: 0.6294\n",
      "Epoch 4/4\n",
      "8192/8192 [==============================] - 2s 183us/step - loss: 1.1765 - accuracy: 0.6407\n",
      "Epoch 1/4\n",
      "8192/8192 [==============================] - 2s 188us/step - loss: 1.5557 - accuracy: 0.5593\n",
      "Epoch 2/4\n",
      "8192/8192 [==============================] - 2s 186us/step - loss: 1.3913 - accuracy: 0.5969\n",
      "Epoch 3/4\n",
      "8192/8192 [==============================] - 1s 183us/step - loss: 1.2783 - accuracy: 0.6262\n",
      "Epoch 4/4\n",
      "8192/8192 [==============================] - 2s 192us/step - loss: 1.1715 - accuracy: 0.6504\n",
      "Epoch 1/4\n",
      "8192/8192 [==============================] - 1s 183us/step - loss: 1.5421 - accuracy: 0.5547\n",
      "Epoch 2/4\n",
      "8192/8192 [==============================] - 1s 182us/step - loss: 1.3656 - accuracy: 0.5955\n",
      "Epoch 3/4\n",
      "8192/8192 [==============================] - 2s 204us/step - loss: 1.2408 - accuracy: 0.6331\n",
      "Epoch 4/4\n",
      "8192/8192 [==============================] - 2s 199us/step - loss: 1.1611 - accuracy: 0.6526\n",
      "Epoch 1/4\n",
      "8192/8192 [==============================] - 2s 201us/step - loss: 1.5514 - accuracy: 0.5640\n",
      "Epoch 2/4\n",
      "8192/8192 [==============================] - 2s 197us/step - loss: 1.3600 - accuracy: 0.6039\n",
      "Epoch 3/4\n",
      "8192/8192 [==============================] - 2s 200us/step - loss: 1.2407 - accuracy: 0.6387\n",
      "Epoch 4/4\n",
      "8192/8192 [==============================] - 2s 213us/step - loss: 1.1667 - accuracy: 0.6533\n",
      "Epoch 1/4\n",
      "8192/8192 [==============================] - 2s 198us/step - loss: 1.5504 - accuracy: 0.5586\n",
      "Epoch 2/4\n",
      "8192/8192 [==============================] - 2s 202us/step - loss: 1.3692 - accuracy: 0.6010\n",
      "Epoch 3/4\n",
      "8192/8192 [==============================] - 2s 195us/step - loss: 1.2679 - accuracy: 0.6240\n",
      "Epoch 4/4\n",
      "8192/8192 [==============================] - 1s 179us/step - loss: 1.1744 - accuracy: 0.6458\n",
      "Epoch 1/4\n",
      "8192/8192 [==============================] - 1s 180us/step - loss: 1.5473 - accuracy: 0.5542\n",
      "Epoch 2/4\n",
      "8192/8192 [==============================] - 1s 181us/step - loss: 1.3669 - accuracy: 0.5963\n",
      "Epoch 3/4\n",
      "8192/8192 [==============================] - 1s 183us/step - loss: 1.2525 - accuracy: 0.6212\n",
      "Epoch 4/4\n",
      "8192/8192 [==============================] - 1s 180us/step - loss: 1.1692 - accuracy: 0.6462\n",
      "Epoch 1/4\n",
      "8192/8192 [==============================] - 1s 182us/step - loss: 1.5247 - accuracy: 0.5649\n",
      "Epoch 2/4\n",
      "8192/8192 [==============================] - 1s 181us/step - loss: 1.3424 - accuracy: 0.6010\n",
      "Epoch 3/4\n",
      "8192/8192 [==============================] - 1s 181us/step - loss: 1.2382 - accuracy: 0.6281\n",
      "Epoch 4/4\n",
      "8192/8192 [==============================] - 1s 181us/step - loss: 1.1417 - accuracy: 0.6553\n",
      "Epoch 1/4\n",
      "8192/8192 [==============================] - 1s 181us/step - loss: 1.5300 - accuracy: 0.5596\n",
      "Epoch 2/4\n",
      "8192/8192 [==============================] - 2s 184us/step - loss: 1.3539 - accuracy: 0.5963\n",
      "Epoch 3/4\n",
      "8192/8192 [==============================] - 1s 183us/step - loss: 1.2379 - accuracy: 0.6306\n",
      "Epoch 4/4\n",
      "8192/8192 [==============================] - 2s 189us/step - loss: 1.1494 - accuracy: 0.6497\n",
      "Epoch 1/4\n",
      "8192/8192 [==============================] - 2s 190us/step - loss: 1.5257 - accuracy: 0.5590\n",
      "Epoch 2/4\n",
      "8192/8192 [==============================] - 2s 203us/step - loss: 1.3488 - accuracy: 0.6042\n",
      "Epoch 3/4\n",
      "8192/8192 [==============================] - 2s 190us/step - loss: 1.2365 - accuracy: 0.6301\n",
      "Epoch 4/4\n",
      "8192/8192 [==============================] - 2s 198us/step - loss: 1.1558 - accuracy: 0.6521\n",
      "Epoch 1/4\n",
      "6306/6306 [==============================] - 1s 192us/step - loss: 1.5196 - accuracy: 0.5630\n",
      "Epoch 2/4\n",
      "6306/6306 [==============================] - 1s 200us/step - loss: 1.3340 - accuracy: 0.6039\n",
      "Epoch 3/4\n",
      "6306/6306 [==============================] - 1s 200us/step - loss: 1.2174 - accuracy: 0.6364\n",
      "Epoch 4/4\n",
      "6306/6306 [==============================] - 1s 219us/step - loss: 1.1035 - accuracy: 0.6694\n"
     ]
    }
   ],
   "source": [
    "for begin, end in batches:\n",
    "    x_batch, y_batch = load_batch(X_train, Y_train, begin, end)\n",
    "    model.fit(x_batch, y_batch, batch_size=256, epochs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"weights_char_rnn.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sampler import CharacterSampler\n",
    "sampler = CharacterSampler(model, chars, SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i like you half  \n",
      "  \n",
      "Where we ala, gaameloot  \n",
      "And it  \n",
      "'Cause I wouldn't know I need happy  \n",
      "Walk on her in my that's the star nare  \n",
      "Lasty farer and taI no on to  \n",
      "The soul don't lang  \n",
      "We on my stuld and wait  \n",
      "I'll be cottone here  \n",
      "Leavin' a little little walked and on a waane stod it all  \n",
      "When you're fur  \n",
      "Yes I'm this with you at amm  \n",
      "But but mysnef it didn't toon's the best tound of tome on  \n",
      "Nowne  \n",
      "When you come to tell beding  \n",
      "And when I could elent at I love some little bittle bit'm a little mishere, all on you beto eneed ot sleep alless will me  \n",
      "  \n",
      "Where we'll happy Hadmalatan words so tony ocess me ats me not, some time in the country knows that you reallt me betur beltor can lat\n"
     ]
    }
   ],
   "source": [
    "print(sampler.sample(\"i like\", 700))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019.3348552498442\n"
     ]
    }
   ],
   "source": [
    "perplexity = 0.0\n",
    "for x, y in list(zip(X_dev, Y_dev))[:800]:\n",
    "    enc_seq = util.one_hot_encode_sequence(x, char2idx).reshape(1, SEQUENCE_LENGTH, vocab_size)\n",
    "    preds = model.predict(enc_seq)\n",
    "    y_hat = idx2char[np.argmax(preds)]\n",
    "    likelihood = preds[0][np.argmax(y)]\n",
    "    perplexity += np.log2(likelihood)\n",
    "perplexity = np.power(2, perplexity * -1/800)\n",
    "print(perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
