{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>song</th>\n",
       "      <th>link</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Ahe's My Kind Of Girl</td>\n",
       "      <td>/a/abba/ahes+my+kind+of+girl_20598417.html</td>\n",
       "      <td>Look at her face, it's a wonderful face  \\nAnd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Andante, Andante</td>\n",
       "      <td>/a/abba/andante+andante_20002708.html</td>\n",
       "      <td>Take it easy with me, please  \\nTouch me gentl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>As Good As New</td>\n",
       "      <td>/a/abba/as+good+as+new_20003033.html</td>\n",
       "      <td>I'll never know why I had to go  \\nWhy I had t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Bang</td>\n",
       "      <td>/a/abba/bang_20598415.html</td>\n",
       "      <td>Making somebody happy is a question of give an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Bang-A-Boomerang</td>\n",
       "      <td>/a/abba/bang+a+boomerang_20002668.html</td>\n",
       "      <td>Making somebody happy is a question of give an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  artist                   song                                        link  \\\n",
       "0   ABBA  Ahe's My Kind Of Girl  /a/abba/ahes+my+kind+of+girl_20598417.html   \n",
       "1   ABBA       Andante, Andante       /a/abba/andante+andante_20002708.html   \n",
       "2   ABBA         As Good As New        /a/abba/as+good+as+new_20003033.html   \n",
       "3   ABBA                   Bang                  /a/abba/bang_20598415.html   \n",
       "4   ABBA       Bang-A-Boomerang      /a/abba/bang+a+boomerang_20002668.html   \n",
       "\n",
       "                                                text  \n",
       "0  Look at her face, it's a wonderful face  \\nAnd...  \n",
       "1  Take it easy with me, please  \\nTouch me gentl...  \n",
       "2  I'll never know why I had to go  \\nWhy I had t...  \n",
       "3  Making somebody happy is a question of give an...  \n",
       "4  Making somebody happy is a question of give an...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"data/songdata.zip\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 76\n"
     ]
    }
   ],
   "source": [
    "vocab = set()\n",
    "for song in df[\"text\"]:\n",
    "    chars = set(song)\n",
    "    vocab = vocab.union(chars)\n",
    "vocab_size = len(vocab)\n",
    "print(\"Vocab size:\", len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = list(vocab)\n",
    "char2idx = { char:i for i,char in enumerate(chars) }\n",
    "idx2char = { i:char for i,char in enumerate(chars) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_samples(song, buffer_length):\n",
    "    tokens = song\n",
    "\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    for i in range(0, len(song)):\n",
    "        if i+buffer_length+1 >= len(tokens):\n",
    "            continue\n",
    "            \n",
    "        x_train.append(tokens[i:i+buffer_length])\n",
    "        y_train.append(tokens[i+buffer_length])\n",
    "\n",
    "    return x_train,y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data length: 133204\n",
      "X[0]: Look a\n",
      "Y[0]: t\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train = [], []\n",
    "for song in df[\"text\"][:100]:\n",
    "    xs, ys = build_samples(song, 6)\n",
    "    x_train.extend(xs)\n",
    "    y_train.extend(ys)\n",
    "print(\"Training data length:\", len(x_train))\n",
    "print(\"X[0]:\", x_train[0])\n",
    "print(\"Y[0]:\", y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "\n",
    "# 80% Train, 10% Dev, 10% Test\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(x_train, y_train, test_size=0.2)\n",
    "X_dev, X_test, Y_dev, Y_test = train_test_split(X_test, Y_test, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import shuffle\n",
    "import util\n",
    "\n",
    "SEQUENCE_LENGTH = 6\n",
    "\n",
    "def generate_batches(data_length, mini_batch_size):\n",
    "    for begin in range(0, data_length, mini_batch_size):\n",
    "        end = min(begin + mini_batch_size, data_length)\n",
    "        yield begin, end\n",
    "\n",
    "def load_batch(xs, ys, begin, end):\n",
    "    batch_size = end-begin\n",
    "    \n",
    "    x_train = np.zeros((batch_size, SEQUENCE_LENGTH, vocab_size))\n",
    "    y_train = np.zeros((batch_size, vocab_size))\n",
    "    \n",
    "    xs_batch = xs[begin:end]\n",
    "    ys_batch = ys[begin:end]\n",
    "    \n",
    "    c = list(zip(xs_batch, ys_batch))\n",
    "    shuffle(c)\n",
    "    xs_batch, ys_batch = zip(*c)\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        x_train[i] = util.one_hot_encode_sequence(xs_batch[i], char2idx)\n",
    "        y_train[i] = util.one_hot_encode(ys_batch[i], char2idx)\n",
    "    \n",
    "    return x_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 6, 76) (512, 76)\n"
     ]
    }
   ],
   "source": [
    "batches = generate_batches(len(X_train), 512)\n",
    "begin, end = next(batches)\n",
    "x_batch, y_batch = load_batch(X_train, Y_train, begin, end)\n",
    "print(x_batch.shape, y_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 'rted  ' -> \n",
      "\n",
      "1 'n  \n",
      "It' -> '\n",
      "2 ' \n",
      "Sorr' -> y\n",
      "3 ' calls' ->  \n",
      "4 ' \n",
      "  \n",
      "Y' -> o\n",
      "5 'have a' ->  \n",
      "6 'ound  ' -> \n",
      "\n",
      "7 'rilyn ' -> F\n",
      "8 'e  \n",
      "Do' ->  \n",
      "9 'find m' -> e\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    xs = ''.join(util.one_hot_decode_sequence(x_batch[i], idx2char))\n",
    "    y = util.one_hot_decode(y_batch[i], idx2char)\n",
    "    print(f\"{i} '{xs}' -> {y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded!\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_4 (LSTM)                (None, 512)               1206272   \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 76)                38988     \n",
      "=================================================================\n",
      "Total params: 1,245,260\n",
      "Trainable params: 1,245,260\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Embedding, Dropout, GRU\n",
    "from keras.layers import LeakyReLU\n",
    "from pathlib import Path\n",
    "\n",
    "def build_model(vocab_size):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(512, input_shape=(SEQUENCE_LENGTH, vocab_size)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    model.add(Dense(vocab_size, activation='softmax'))\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer=\"adam\", metrics = ['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = build_model(vocab_size)\n",
    "weights_path = Path(\"weights_char_rnn.h5\")\n",
    "if weights_path.is_file():\n",
    "    print(\"Loaded!\")\n",
    "    model.load_weights(weights_path.resolve())\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = generate_batches(len(X_train), 4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "4096/4096 [==============================] - 1s 182us/step - loss: 1.1827 - accuracy: 0.6453\n",
      "Epoch 2/4\n",
      "4096/4096 [==============================] - 1s 201us/step - loss: 0.8911 - accuracy: 0.7251\n",
      "Epoch 3/4\n",
      "4096/4096 [==============================] - 1s 214us/step - loss: 0.7282 - accuracy: 0.7742\n",
      "Epoch 4/4\n",
      "4096/4096 [==============================] - 1s 201us/step - loss: 0.6183 - accuracy: 0.8037\n",
      "Epoch 1/4\n",
      "4096/4096 [==============================] - 1s 209us/step - loss: 1.1492 - accuracy: 0.6650\n",
      "Epoch 2/4\n",
      "4096/4096 [==============================] - 1s 221us/step - loss: 0.8636 - accuracy: 0.7307\n",
      "Epoch 3/4\n",
      "4096/4096 [==============================] - 1s 220us/step - loss: 0.7162 - accuracy: 0.7788\n",
      "Epoch 4/4\n",
      "4096/4096 [==============================] - 1s 211us/step - loss: 0.6301 - accuracy: 0.8027\n",
      "Epoch 1/4\n",
      "4096/4096 [==============================] - 1s 210us/step - loss: 1.1366 - accuracy: 0.6650\n",
      "Epoch 2/4\n",
      "4096/4096 [==============================] - 1s 211us/step - loss: 0.8655 - accuracy: 0.7285\n",
      "Epoch 3/4\n",
      "4096/4096 [==============================] - 1s 206us/step - loss: 0.7340 - accuracy: 0.7727\n",
      "Epoch 4/4\n",
      "4096/4096 [==============================] - 1s 189us/step - loss: 0.6342 - accuracy: 0.8064\n",
      "Epoch 1/4\n",
      "4096/4096 [==============================] - 1s 192us/step - loss: 1.1131 - accuracy: 0.6626\n",
      "Epoch 2/4\n",
      "4096/4096 [==============================] - 1s 193us/step - loss: 0.8252 - accuracy: 0.7390\n",
      "Epoch 3/4\n",
      "4096/4096 [==============================] - 1s 196us/step - loss: 0.6889 - accuracy: 0.7834\n",
      "Epoch 4/4\n",
      "4096/4096 [==============================] - 1s 195us/step - loss: 0.5922 - accuracy: 0.8164\n",
      "Epoch 1/4\n",
      "4096/4096 [==============================] - 1s 207us/step - loss: 1.1507 - accuracy: 0.6621\n",
      "Epoch 2/4\n",
      "4096/4096 [==============================] - 1s 208us/step - loss: 0.8790 - accuracy: 0.7217\n",
      "Epoch 3/4\n",
      "4096/4096 [==============================] - 1s 208us/step - loss: 0.7098 - accuracy: 0.7710\n",
      "Epoch 4/4\n",
      "4096/4096 [==============================] - 1s 206us/step - loss: 0.6123 - accuracy: 0.8105\n",
      "Epoch 1/4\n",
      "4096/4096 [==============================] - 1s 205us/step - loss: 1.1033 - accuracy: 0.6743\n",
      "Epoch 2/4\n",
      "4096/4096 [==============================] - 1s 202us/step - loss: 0.8233 - accuracy: 0.7354\n",
      "Epoch 3/4\n",
      "4096/4096 [==============================] - 1s 201us/step - loss: 0.6850 - accuracy: 0.7874\n",
      "Epoch 4/4\n",
      "4096/4096 [==============================] - 1s 199us/step - loss: 0.5753 - accuracy: 0.8218\n",
      "Epoch 1/4\n",
      "4096/4096 [==============================] - 1s 207us/step - loss: 1.1483 - accuracy: 0.6616\n",
      "Epoch 2/4\n",
      "4096/4096 [==============================] - 1s 205us/step - loss: 0.8694 - accuracy: 0.7261\n",
      "Epoch 3/4\n",
      "4096/4096 [==============================] - 1s 208us/step - loss: 0.7126 - accuracy: 0.7771\n",
      "Epoch 4/4\n",
      "4096/4096 [==============================] - 1s 208us/step - loss: 0.6131 - accuracy: 0.8062\n",
      "Epoch 1/4\n",
      "4096/4096 [==============================] - 1s 202us/step - loss: 1.0917 - accuracy: 0.6655\n",
      "Epoch 2/4\n",
      "4096/4096 [==============================] - 1s 206us/step - loss: 0.8183 - accuracy: 0.7422\n",
      "Epoch 3/4\n",
      "4096/4096 [==============================] - 1s 210us/step - loss: 0.6758 - accuracy: 0.7839\n",
      "Epoch 4/4\n",
      "4096/4096 [==============================] - 1s 206us/step - loss: 0.5886 - accuracy: 0.8176\n",
      "Epoch 1/4\n",
      "4096/4096 [==============================] - 1s 210us/step - loss: 1.1448 - accuracy: 0.6562\n",
      "Epoch 2/4\n",
      "4096/4096 [==============================] - 1s 206us/step - loss: 0.8553 - accuracy: 0.7341\n",
      "Epoch 3/4\n",
      "4096/4096 [==============================] - 1s 210us/step - loss: 0.7024 - accuracy: 0.7830\n",
      "Epoch 4/4\n",
      "4096/4096 [==============================] - 1s 206us/step - loss: 0.6056 - accuracy: 0.8105\n",
      "Epoch 1/4\n",
      "4096/4096 [==============================] - 1s 206us/step - loss: 1.1037 - accuracy: 0.6665\n",
      "Epoch 2/4\n",
      "4096/4096 [==============================] - 1s 211us/step - loss: 0.8269 - accuracy: 0.7380\n",
      "Epoch 3/4\n",
      "4096/4096 [==============================] - 1s 203us/step - loss: 0.6932 - accuracy: 0.7773\n",
      "Epoch 4/4\n",
      "4096/4096 [==============================] - 1s 205us/step - loss: 0.5855 - accuracy: 0.8176\n",
      "Epoch 1/4\n",
      "4096/4096 [==============================] - 1s 208us/step - loss: 1.1146 - accuracy: 0.6633\n",
      "Epoch 2/4\n",
      "4096/4096 [==============================] - 1s 209us/step - loss: 0.8262 - accuracy: 0.7380\n",
      "Epoch 3/4\n",
      "4096/4096 [==============================] - 1s 204us/step - loss: 0.6889 - accuracy: 0.7822\n",
      "Epoch 4/4\n",
      "4096/4096 [==============================] - 1s 203us/step - loss: 0.5926 - accuracy: 0.8145\n",
      "Epoch 1/4\n",
      "4096/4096 [==============================] - 1s 218us/step - loss: 1.0933 - accuracy: 0.6648\n",
      "Epoch 2/4\n",
      "4096/4096 [==============================] - 1s 214us/step - loss: 0.8286 - accuracy: 0.7373\n",
      "Epoch 3/4\n",
      "4096/4096 [==============================] - 1s 194us/step - loss: 0.6746 - accuracy: 0.7891\n",
      "Epoch 4/4\n",
      "4096/4096 [==============================] - 1s 193us/step - loss: 0.5781 - accuracy: 0.8218\n",
      "Epoch 1/4\n",
      "4096/4096 [==============================] - 1s 192us/step - loss: 1.1076 - accuracy: 0.6711\n",
      "Epoch 2/4\n",
      "4096/4096 [==============================] - 1s 191us/step - loss: 0.8202 - accuracy: 0.7429\n",
      "Epoch 3/4\n",
      "4096/4096 [==============================] - 1s 195us/step - loss: 0.6886 - accuracy: 0.7822\n",
      "Epoch 4/4\n",
      "4096/4096 [==============================] - 1s 198us/step - loss: 0.5900 - accuracy: 0.8167\n",
      "Epoch 1/4\n",
      "4096/4096 [==============================] - 1s 195us/step - loss: 1.1404 - accuracy: 0.6606\n",
      "Epoch 2/4\n",
      "4096/4096 [==============================] - 1s 189us/step - loss: 0.8439 - accuracy: 0.7302\n",
      "Epoch 3/4\n",
      "4096/4096 [==============================] - 1s 186us/step - loss: 0.6821 - accuracy: 0.7871\n",
      "Epoch 4/4\n",
      "4096/4096 [==============================] - 1s 190us/step - loss: 0.5914 - accuracy: 0.8152\n",
      "Epoch 1/4\n",
      "4096/4096 [==============================] - 1s 186us/step - loss: 1.1347 - accuracy: 0.6641\n",
      "Epoch 2/4\n",
      "4096/4096 [==============================] - 1s 202us/step - loss: 0.8329 - accuracy: 0.7368\n",
      "Epoch 3/4\n",
      "4096/4096 [==============================] - 1s 198us/step - loss: 0.6828 - accuracy: 0.7913\n",
      "Epoch 4/4\n",
      "4096/4096 [==============================] - 1s 202us/step - loss: 0.5811 - accuracy: 0.8193\n",
      "Epoch 1/4\n",
      "4096/4096 [==============================] - 1s 205us/step - loss: 1.1182 - accuracy: 0.6648\n",
      "Epoch 2/4\n",
      "4096/4096 [==============================] - 1s 210us/step - loss: 0.8417 - accuracy: 0.7341\n",
      "Epoch 3/4\n",
      "4096/4096 [==============================] - 1s 207us/step - loss: 0.7026 - accuracy: 0.7776\n",
      "Epoch 4/4\n",
      "4096/4096 [==============================] - 1s 198us/step - loss: 0.6133 - accuracy: 0.8042\n",
      "Epoch 1/4\n",
      "4096/4096 [==============================] - 1s 191us/step - loss: 1.1337 - accuracy: 0.6516\n",
      "Epoch 2/4\n",
      "4096/4096 [==============================] - 1s 186us/step - loss: 0.8477 - accuracy: 0.7263\n",
      "Epoch 3/4\n",
      "4096/4096 [==============================] - 1s 194us/step - loss: 0.6972 - accuracy: 0.7771\n",
      "Epoch 4/4\n",
      "4096/4096 [==============================] - 1s 202us/step - loss: 0.6194 - accuracy: 0.8005\n",
      "Epoch 1/4\n",
      "4096/4096 [==============================] - 1s 202us/step - loss: 1.0748 - accuracy: 0.6736\n",
      "Epoch 2/4\n",
      "4096/4096 [==============================] - 1s 205us/step - loss: 0.8355 - accuracy: 0.7434\n",
      "Epoch 3/4\n",
      "4096/4096 [==============================] - 1s 209us/step - loss: 0.6819 - accuracy: 0.7917\n",
      "Epoch 4/4\n",
      "4096/4096 [==============================] - 1s 203us/step - loss: 0.5974 - accuracy: 0.8120\n",
      "Epoch 1/4\n",
      "4096/4096 [==============================] - 1s 203us/step - loss: 1.1094 - accuracy: 0.6646\n",
      "Epoch 2/4\n",
      "4096/4096 [==============================] - 1s 207us/step - loss: 0.8067 - accuracy: 0.7490\n",
      "Epoch 3/4\n",
      "4096/4096 [==============================] - 1s 201us/step - loss: 0.6640 - accuracy: 0.7852\n",
      "Epoch 4/4\n",
      "4096/4096 [==============================] - 1s 204us/step - loss: 0.5738 - accuracy: 0.8240\n",
      "Epoch 1/4\n",
      "4096/4096 [==============================] - 1s 206us/step - loss: 1.0829 - accuracy: 0.6794\n",
      "Epoch 2/4\n",
      "4096/4096 [==============================] - 1s 208us/step - loss: 0.8251 - accuracy: 0.7422\n",
      "Epoch 3/4\n",
      "4096/4096 [==============================] - 1s 214us/step - loss: 0.6552 - accuracy: 0.7993\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4096/4096 [==============================] - 1s 203us/step - loss: 0.5597 - accuracy: 0.8284\n",
      "Epoch 1/4\n",
      "4096/4096 [==============================] - 1s 213us/step - loss: 1.0898 - accuracy: 0.6672\n",
      "Epoch 2/4\n",
      "4096/4096 [==============================] - 1s 191us/step - loss: 0.8028 - accuracy: 0.7500\n",
      "Epoch 3/4\n",
      "4096/4096 [==============================] - 1s 189us/step - loss: 0.6541 - accuracy: 0.7927\n",
      "Epoch 4/4\n",
      "4096/4096 [==============================] - 1s 188us/step - loss: 0.5670 - accuracy: 0.8225\n",
      "Epoch 1/4\n",
      "4096/4096 [==============================] - 1s 188us/step - loss: 1.0789 - accuracy: 0.6670\n",
      "Epoch 2/4\n",
      "4096/4096 [==============================] - 1s 190us/step - loss: 0.7973 - accuracy: 0.7490\n",
      "Epoch 3/4\n",
      "4096/4096 [==============================] - 1s 188us/step - loss: 0.6406 - accuracy: 0.7944\n",
      "Epoch 4/4\n",
      "4096/4096 [==============================] - 1s 203us/step - loss: 0.5604 - accuracy: 0.8218\n",
      "Epoch 1/4\n",
      "4096/4096 [==============================] - 1s 200us/step - loss: 1.1293 - accuracy: 0.6614\n",
      "Epoch 2/4\n",
      "4096/4096 [==============================] - 1s 211us/step - loss: 0.8138 - accuracy: 0.7437\n",
      "Epoch 3/4\n",
      "4096/4096 [==============================] - 1s 217us/step - loss: 0.6700 - accuracy: 0.7791\n",
      "Epoch 4/4\n",
      "4096/4096 [==============================] - 1s 192us/step - loss: 0.5791 - accuracy: 0.8132\n",
      "Epoch 1/4\n",
      "4096/4096 [==============================] - 1s 195us/step - loss: 1.0778 - accuracy: 0.6763\n",
      "Epoch 2/4\n",
      "4096/4096 [==============================] - 1s 193us/step - loss: 0.7689 - accuracy: 0.7625\n",
      "Epoch 3/4\n",
      "4096/4096 [==============================] - 1s 180us/step - loss: 0.6518 - accuracy: 0.7896\n",
      "Epoch 4/4\n",
      "4096/4096 [==============================] - 1s 183us/step - loss: 0.5498 - accuracy: 0.8291\n",
      "Epoch 1/4\n",
      "4096/4096 [==============================] - 1s 200us/step - loss: 1.1151 - accuracy: 0.6707\n",
      "Epoch 2/4\n",
      "4096/4096 [==============================] - 1s 211us/step - loss: 0.8235 - accuracy: 0.7495\n",
      "Epoch 3/4\n",
      "4096/4096 [==============================] - 1s 182us/step - loss: 0.6750 - accuracy: 0.7949\n",
      "Epoch 4/4\n",
      "4096/4096 [==============================] - 1s 179us/step - loss: 0.5684 - accuracy: 0.8220\n",
      "Epoch 1/4\n",
      "4096/4096 [==============================] - 1s 199us/step - loss: 1.1258 - accuracy: 0.6660\n",
      "Epoch 2/4\n",
      "4096/4096 [==============================] - 1s 214us/step - loss: 0.8019 - accuracy: 0.7466\n",
      "Epoch 3/4\n",
      "4096/4096 [==============================] - 1s 221us/step - loss: 0.6645 - accuracy: 0.7976\n",
      "Epoch 4/4\n",
      "4096/4096 [==============================] - 1s 225us/step - loss: 0.5626 - accuracy: 0.8220\n",
      "Epoch 1/4\n",
      "67/67 [==============================] - 0s 343us/step - loss: 0.4909 - accuracy: 0.8358\n",
      "Epoch 2/4\n",
      "67/67 [==============================] - 0s 395us/step - loss: 0.3908 - accuracy: 0.8806\n",
      "Epoch 3/4\n",
      "67/67 [==============================] - 0s 429us/step - loss: 0.3759 - accuracy: 0.8806\n",
      "Epoch 4/4\n",
      "67/67 [==============================] - 0s 371us/step - loss: 0.2673 - accuracy: 0.9254\n"
     ]
    }
   ],
   "source": [
    "for begin, end in batches:\n",
    "    x_batch, y_batch = load_batch(X_train, Y_train, begin, end)\n",
    "    model.fit(x_batch, y_batch, batch_size=256, epochs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"weights_char_rnn.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i like a star  \n",
      "To fill the now that sick I will never the night  \n",
      "And the firapisey  \n",
      "It's gonna find the sun is the derd in the dry  \n",
      "  \n",
      "Enele toonn everything on the pratting aby tonel wey how I well lay love me will the day  \n",
      "  \n",
      "Just as look  \n",
      "Looking down  \n",
      "The amriting for tros  \n",
      "sny that ald the  \n",
      "Stinr you  \n",
      "So evoe this dingas same  \n",
      "  \n",
      "Just as longhr now the rock'n Roller  \n",
      "And the sosdy I chalfam  \n",
      "  \n",
      "(Pick a bale a day  \n",
      "  \n",
      "Sometom the head to gan  \n",
      "The are the dancing Queen, diding how I'n still foreoun  \n",
      "You peap fert you go love of ours and freet, night  \n",
      "  \n",
      "As I surr is froany  \n",
      "To the bus  \n",
      "The sing  \n",
      "One doo  \n",
      "(Therw in tog the goonn got to me  \n",
      "  \n",
      "For and you long tine war  tand\n"
     ]
    }
   ],
   "source": [
    "seq = \"i like\"\n",
    "text = seq + \"\"\n",
    "for i in range(700):\n",
    "    enc_seq = util.one_hot_encode_sequence(seq, char2idx).reshape(1, SEQUENCE_LENGTH, vocab_size)\n",
    "    preds = model.predict(enc_seq)\n",
    "    next_char = idx2char[util.sample(preds)]\n",
    "    text += next_char\n",
    "    seq = seq[1:]\n",
    "    seq += next_char\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examples\n",
    "i like you on my omtowe belnet thing\n",
    "i like you smertbees the sperid, the"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i like  \n",
    "Lav alllithy ist tive comemongen,  \n",
    "Bot nrain  moting wipded, plqie tone, hagfue inght ir goint we lingery  \n",
    "Pookeine mf you of a bit)  \n",
    "We world yourvew on the Krrowous goneven the titlrus risting in the frid you preaon  \n",
    "Don't seel) tark nopceer and I had thk rtce smige wnel  \n",
    "But and eveninge that ceres my momingod aplond thillubn wasto longs gow as sictuplist thum Dild my fingert  sBie one you when I've met at )  \n",
    " yaherr I'm just con't kere I bobid you lyokich amw svenaken  \n",
    "The wey for the nights little and ortroc  \n",
    "Where is not bitner-belesdd  \n",
    "Farmar ands to sleas no cachas yours not sol-endr comd flide oh've wlike a jumcabe this lotely to find my wamas aread  \n",
    "On bofflr whe  \n",
    "E s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i like Conltare  \n",
    "Looking for the knaw, goonb folway  \n",
    "You'll bit's -owing wes down  \n",
    "(I'  \n",
    "Uilithe  \n",
    "In a thannt, ngis lovelight  \n",
    "come on wosta finky  \n",
    "Plyinm lowing mepes the nean you spen alonn  \n",
    "  \n",
    "Bet a amias-e canot reonef by me in oll fand ther for sther  \n",
    "Plpaneb and friend -obed be diwan needs \n",
    " of \n",
    "  \n",
    "Nine, pingeen is ther \"like the siredrah haning my bindy ond tryet sild 't lound here I  \n",
    "I dunne  \n",
    "Ehey bees wnite  \n",
    "The rock'n roll  \n",
    "To fell on the durktion  \n",
    "But in my awning  \n",
    "Ith  \n",
    "Every that spanet stige  \n",
    "Honey honey honey hene  \n",
    "And belinve  \n",
    "Sometomerow, thas bidyth-te de,rida, I got to step ty with a  \n",
    "Long dance  \n",
    "Swintgres  \n",
    "You stall  \n",
    "In't the ny heand to was bock had a fais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i like a star  \n",
    "To fill the now that sick I will never the night  \n",
    "And the firapisey  \n",
    "It's gonna find the sun is the derd in the dry  \n",
    "  \n",
    "Enele toonn everything on the pratting aby tonel wey how I well lay love me will the day  \n",
    "  \n",
    "Just as look  \n",
    "Looking down  \n",
    "The amriting for tros  \n",
    "sny that ald the  \n",
    "Stinr you  \n",
    "So evoe this dingas same  \n",
    "  \n",
    "Just as longhr now the rock'n Roller  \n",
    "And the sosdy I chalfam  \n",
    "  \n",
    "(Pick a bale a day  \n",
    "  \n",
    "Sometom the head to gan  \n",
    "The are the dancing Queen, diding how I'n still foreoun  \n",
    "You peap fert you go love of ours and freet, night  \n",
    "  \n",
    "As I surr is froany  \n",
    "To the bus  \n",
    "The sing  \n",
    "One doo  \n",
    "(Therw in tog the goonn got to me  \n",
    "  \n",
    "For and you long tine war  tand"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
