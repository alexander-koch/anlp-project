{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "w2v = KeyedVectors.load_word2vec_format('glove.6B.100d.bin.word2vec', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "SEQUENCE_LENGTH = 6\n",
    "HIDDEN_SIZE = 256\n",
    "\n",
    "EMBEDDING_SIZE_ORIG = 100\n",
    "EMBEDDING_SIZE = 103\n",
    "\n",
    "def encode_word(word, w2v):\n",
    "    if word == \"<pad>\":\n",
    "        v = np.zeros((EMBEDDING_SIZE,))\n",
    "        v[EMBEDDING_SIZE-1] = 1\n",
    "        return v\n",
    "    elif word == \"<newline>\":\n",
    "        v = np.zeros((EMBEDDING_SIZE,))\n",
    "        v[EMBEDDING_SIZE-2] = 1\n",
    "        return v\n",
    "    elif word == \"<unk>\" or word not in w2v:\n",
    "        v = np.zeros((EMBEDDING_SIZE,))\n",
    "        v[EMBEDDING_SIZE-3] = 1\n",
    "        return v\n",
    "    else:        \n",
    "        v = w2v[word]\n",
    "        w = np.zeros((3,))\n",
    "        return np.append(v, w, axis=0)\n",
    "\n",
    "def encode_words(words, w2v):\n",
    "    vec = np.zeros((len(words), EMBEDDING_SIZE))\n",
    "    for (i,word) in enumerate(words):\n",
    "        vec[i] = encode_word(word, w2v)\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(103,)\n",
      "(103,)\n"
     ]
    }
   ],
   "source": [
    "v = encode_word(\"hello\", w2v)\n",
    "print(v.shape)\n",
    "w = encode_word(\"boom-a-boomerang\", w2v)\n",
    "print(w.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101\n",
      "102\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "v_newline = encode_word(\"<newline>\", w2v)\n",
    "v_pad = encode_word(\"<pad>\", w2v)\n",
    "v_unk = encode_word(\"<unk>\", w2v)\n",
    "\n",
    "print(np.argmax(v_newline))\n",
    "print(np.argmax(v_pad))\n",
    "print(np.argmax(v_unk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_vec(vec, w2v):\n",
    "    base_vec = vec[:EMBEDDING_SIZE_ORIG]\n",
    "    ext_vec = vec[EMBEDDING_SIZE_ORIG:]\n",
    "    if ext_vec[0]:\n",
    "        return \"<unk>\"\n",
    "    elif ext_vec[1]:\n",
    "        return \"<newline>\"\n",
    "    elif ext_vec[2]:\n",
    "        return \"<pad>\"\n",
    "    else:\n",
    "        return w2v.similar_by_vector(base_vec)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      "<newline>\n",
      "<pad>\n",
      "<unk>\n"
     ]
    }
   ],
   "source": [
    "print(decode_vec(v, w2v))\n",
    "print(decode_vec(v_newline, w2v))\n",
    "print(decode_vec(v_pad, w2v))\n",
    "print(decode_vec(v_unk, w2v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_song(song, buffer_length):\n",
    "    tokens = song\n",
    "\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    for i in range(0, len(song)):\n",
    "        if i+buffer_length+1 >= len(tokens):\n",
    "            pad_length = (i+buffer_length+1) - len(tokens)\n",
    "            tokens += ['<pad>'] * pad_length\n",
    "\n",
    "        x_train.append(tokens[i:i+buffer_length])\n",
    "        y_train.append(tokens[i+buffer_length])\n",
    "\n",
    "    return x_train,y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_vocab = {'<pad>', '<unk>'}\n",
    "songs = []\n",
    "with open(\"data/sentences.txt\", \"r\") as f:\n",
    "    for line in f.readlines():\n",
    "        tokens = [token for token in line.rstrip().split(\" \")]\n",
    "        songs.append(tokens)\n",
    "        token_vocab = token_vocab.union(set(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114929\n",
      "['look', 'at', 'her', 'face', ',', 'it']\n"
     ]
    }
   ],
   "source": [
    "x_vec = []\n",
    "y_vec = []\n",
    "for song in songs:\n",
    "    x_vec_i, y_vec_i = tokenize_song(song, SEQUENCE_LENGTH)\n",
    "    x_vec.extend(x_vec_i)\n",
    "    y_vec.extend(y_vec_i)\n",
    "print(len(x_vec))\n",
    "print(x_vec[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 5111\n",
      "W2V vocab size: 400000\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def load_vocab(path):\n",
    "    vocab = list()\n",
    "    with path.open(\"r\") as f:\n",
    "        for line in f.readlines():\n",
    "            vocab.append(line.rstrip())\n",
    "    return vocab\n",
    "\n",
    "def write_vocab(path):\n",
    "    with path.open(\"w\") as f:\n",
    "        for word in words:\n",
    "            f.write(word + \"\\n\")\n",
    "        \n",
    "# Read or create vocab path\n",
    "vocab_path = Path(\"vocab.txt\")\n",
    "words = list()\n",
    "if vocab_path.is_file():\n",
    "    words = load_vocab(vocab_path)   \n",
    "else:\n",
    "    write_vocab(vocab_path)\n",
    "    words = list(token_vocab)\n",
    "\n",
    "vocab_size = len(words)\n",
    "print(\"Vocab size:\", vocab_size)\n",
    "print(\"W2V vocab size:\", len(w2v.vocab))\n",
    "word2idx = { word:i for i,word in enumerate(words) }\n",
    "idx2word = { i:word for i,word in enumerate(words) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(word, word2idx):\n",
    "    v = np.zeros((len(word2idx, )))\n",
    "    v[word2idx[word]] = 1\n",
    "    return v\n",
    "\n",
    "def one_hot_decode(word, idx2word):\n",
    "    return idx2word[np.argmax(word)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1448\n",
      "1448\n"
     ]
    }
   ],
   "source": [
    "print(word2idx[\"hello\"])\n",
    "print(np.argmax(one_hot_encode(\"hello\", word2idx)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "\n",
    "# 80% Train, 10% Dev, 10% Test\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(x_vec, y_vec, test_size=0.2)\n",
    "X_dev, X_test, Y_dev, Y_test = train_test_split(X_test, Y_test, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size: 114929\n",
      "Training size: 91943\n",
      "Development set size: 11493\n",
      "Test set size: 11493\n"
     ]
    }
   ],
   "source": [
    "print(\"Total size:\", len(x_vec))\n",
    "print(\"Training size:\", len(X_train))\n",
    "print(\"Development set size:\", len(X_dev))\n",
    "print(\"Test set size:\", len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "def generate_batches(data_length, mini_batch_size):\n",
    "    for begin in range(0, data_length, mini_batch_size):\n",
    "        end = min(begin + mini_batch_size, data_length)\n",
    "        yield begin, end\n",
    "\n",
    "def load_batch(xs, ys, begin, end):\n",
    "    batch_size = end-begin\n",
    "    \n",
    "    x_train = np.zeros((batch_size, SEQUENCE_LENGTH, EMBEDDING_SIZE))\n",
    "    y_train = np.zeros((batch_size, vocab_size))\n",
    "    \n",
    "    xs_batch = xs[begin:end]\n",
    "    ys_batch = ys[begin:end]\n",
    "    \n",
    "    c = list(zip(xs_batch, ys_batch))\n",
    "    shuffle(c)\n",
    "    xs_batch, ys_batch = zip(*c)\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        x_train[i] = encode_words(xs_batch[i], w2v)\n",
    "        y_train[i] = one_hot_encode(ys_batch[i], word2idx)\n",
    "    \n",
    "    return x_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 6, 103) (512, 5111)\n",
      "X train\n",
      "they ask for daddy <newline> hey\n",
      "in a free world <newline> i\n",
      "my oasis in the night ,\n",
      "home , <newline> searching for this\n",
      ") <newline> like i always do\n",
      "hey darling ( have to tell\n",
      "baby do n't want me no\n",
      "goes where you can find me\n",
      "day of your life <newline> gon\n",
      "and to receive <newline> for every\n",
      "\n",
      "Y train\n",
      "hey\n",
      "whistle\n",
      "yeah\n",
      "land\n",
      "<newline>\n",
      "you\n",
      "more\n",
      ",\n",
      "na\n",
      "little\n"
     ]
    }
   ],
   "source": [
    "batches = generate_batches(len(X_train), 512)\n",
    "begin, end = next(batches)\n",
    "\n",
    "x_train, y_train = load_batch(X_train, Y_train, begin, end)\n",
    "print(x_train.shape, y_train.shape)\n",
    "\n",
    "print(\"X train\")\n",
    "for j in range(10):\n",
    "    ws = ' '.join([decode_vec(x_train[j][i], w2v) for i in range(SEQUENCE_LENGTH)])\n",
    "    print(ws)\n",
    "    \n",
    "print(\"\\nY train\")\n",
    "for j in range(10):\n",
    "    print(one_hot_decode(y_train[j], idx2word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, LSTM, Embedding, Dropout, GRU\n",
    "from keras.layers import LeakyReLU\n",
    "def build_model1(vocab_size):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(128, input_shape=(SEQUENCE_LENGTH, EMBEDDING_SIZE), return_sequences=True))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(LSTM(1024))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(2048))\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    model.add(Dense(vocab_size, activation='softmax'))\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer=\"adam\", metrics = ['accuracy'])\n",
    "    return model\n",
    "\n",
    "def build_model2(vocab_size):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(512, input_shape=(SEQUENCE_LENGTH, EMBEDDING_SIZE)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    model.add(Dense(vocab_size, activation='softmax'))\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer=\"adam\", metrics = ['accuracy'])\n",
    "    return model\n",
    "\n",
    "model1 = build_model1(vocab_size)\n",
    "model2 = build_model2(vocab_size)\n",
    "models = [model1, model2]\n",
    "for (i, model) in enumerate(models):\n",
    "    weights_path= Path(f\"weights_model{i+1}.h5\")\n",
    "    if weights_path.is_file():\n",
    "        model.load_weights(weights_path.resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batches: 23\n"
     ]
    }
   ],
   "source": [
    "batches = list(generate_batches(len(X_train), 4096))\n",
    "shuffle(batches)\n",
    "print(\"Batches:\", len(batches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: 0\n",
      "Epoch 1/4\n",
      "4096/4096 [==============================] - 4s 1ms/step - loss: 1.7577 - accuracy: 0.5879\n",
      "Epoch 2/4\n",
      "4096/4096 [==============================] - 5s 1ms/step - loss: 0.6195 - accuracy: 0.8247\n",
      "Epoch 3/4\n",
      "4096/4096 [==============================] - 5s 1ms/step - loss: 0.2846 - accuracy: 0.9189\n",
      "Epoch 4/4\n",
      "4096/4096 [==============================] - 5s 1ms/step - loss: 0.1630 - accuracy: 0.9590\n",
      "Epoch 1/4\n",
      "4096/4096 [==============================] - 5s 1ms/step - loss: 1.6868 - accuracy: 0.5969\n",
      "Epoch 2/4\n",
      "4096/4096 [==============================] - 5s 1ms/step - loss: 0.6360 - accuracy: 0.8184\n",
      "Epoch 3/4\n",
      "4096/4096 [==============================] - 5s 1ms/step - loss: 0.2586 - accuracy: 0.9263\n",
      "Epoch 4/4\n",
      "4096/4096 [==============================] - 5s 1ms/step - loss: 0.1749 - accuracy: 0.9490\n",
      "Epoch 1/4\n",
      "4096/4096 [==============================] - 5s 1ms/step - loss: 1.7088 - accuracy: 0.5896\n",
      "Epoch 2/4\n",
      "4096/4096 [==============================] - 5s 1ms/step - loss: 0.6066 - accuracy: 0.8293\n",
      "Epoch 3/4\n",
      "4096/4096 [==============================] - 5s 1ms/step - loss: 0.2780 - accuracy: 0.9229\n",
      "Epoch 4/4\n",
      "4096/4096 [==============================] - 5s 1ms/step - loss: 0.1728 - accuracy: 0.9565\n",
      "Epoch 1/4\n",
      "4096/4096 [==============================] - 5s 1ms/step - loss: 1.6081 - accuracy: 0.6111\n",
      "Epoch 2/4\n",
      "4096/4096 [==============================] - 5s 1ms/step - loss: 0.5925 - accuracy: 0.8286\n",
      "Epoch 3/4\n",
      "4096/4096 [==============================] - 5s 1ms/step - loss: 0.2448 - accuracy: 0.9285\n",
      "Epoch 4/4\n",
      "4096/4096 [==============================] - 5s 1ms/step - loss: 0.1611 - accuracy: 0.9563\n",
      "Epoch 1/4\n",
      "4096/4096 [==============================] - 5s 1ms/step - loss: 1.5917 - accuracy: 0.6086\n",
      "Epoch 2/4\n",
      "4096/4096 [==============================] - 5s 1ms/step - loss: 0.5436 - accuracy: 0.8420\n",
      "Epoch 3/4\n",
      "4096/4096 [==============================] - 4s 1ms/step - loss: 0.2404 - accuracy: 0.9304\n",
      "Epoch 4/4\n",
      "4096/4096 [==============================] - 4s 1ms/step - loss: 0.1398 - accuracy: 0.9624\n",
      "Epoch 1/4\n",
      "4096/4096 [==============================] - 5s 1ms/step - loss: 1.6480 - accuracy: 0.6060\n",
      "Epoch 2/4\n",
      "4096/4096 [==============================] - 5s 1ms/step - loss: 0.5763 - accuracy: 0.8330\n",
      "Epoch 3/4\n",
      "4096/4096 [==============================] - 5s 1ms/step - loss: 0.2543 - accuracy: 0.9255\n",
      "Epoch 4/4\n",
      "4096/4096 [==============================] - 4s 1ms/step - loss: 0.1522 - accuracy: 0.9558\n",
      "Epoch 1/4\n",
      "4096/4096 [==============================] - 4s 1ms/step - loss: 1.6897 - accuracy: 0.6021\n",
      "Epoch 2/4\n",
      "4096/4096 [==============================] - 5s 1ms/step - loss: 0.5848 - accuracy: 0.8325\n",
      "Epoch 3/4\n",
      "4096/4096 [==============================] - 5s 1ms/step - loss: 0.2781 - accuracy: 0.9199\n",
      "Epoch 4/4\n",
      "4096/4096 [==============================] - 5s 1ms/step - loss: 0.1654 - accuracy: 0.9546\n",
      "Epoch 1/4\n",
      "4096/4096 [==============================] - 5s 1ms/step - loss: 1.5849 - accuracy: 0.6223\n",
      "Epoch 2/4\n",
      "4096/4096 [==============================] - 5s 1ms/step - loss: 0.6162 - accuracy: 0.8267\n",
      "Epoch 3/4\n",
      "4096/4096 [==============================] - 5s 1ms/step - loss: 0.2774 - accuracy: 0.9167\n",
      "Epoch 4/4\n",
      "4096/4096 [==============================] - 5s 1ms/step - loss: 0.1670 - accuracy: 0.9512\n",
      "Epoch 1/4\n",
      "4096/4096 [==============================] - 5s 1ms/step - loss: 1.5378 - accuracy: 0.6145\n",
      "Epoch 2/4\n",
      "4096/4096 [==============================] - 5s 1ms/step - loss: 0.5682 - accuracy: 0.8367\n",
      "Epoch 3/4\n",
      "4096/4096 [==============================] - 5s 1ms/step - loss: 0.2486 - accuracy: 0.9246\n",
      "Epoch 4/4\n",
      "4096/4096 [==============================] - 5s 1ms/step - loss: 0.1437 - accuracy: 0.9607\n",
      "Epoch 1/4\n",
      "4096/4096 [==============================] - 5s 1ms/step - loss: 1.5298 - accuracy: 0.6257\n",
      "Epoch 2/4\n",
      "4096/4096 [==============================] - 5s 1ms/step - loss: 0.5693 - accuracy: 0.8342\n",
      "Epoch 3/4\n",
      "4096/4096 [==============================] - 4s 1ms/step - loss: 0.2600 - accuracy: 0.9268\n",
      "Epoch 4/4\n",
      "4096/4096 [==============================] - 5s 1ms/step - loss: 0.1654 - accuracy: 0.9551\n",
      "Epoch 1/4\n",
      "4096/4096 [==============================] - 5s 1ms/step - loss: 1.5443 - accuracy: 0.6221\n",
      "Epoch 2/4\n",
      "4096/4096 [==============================] - 5s 1ms/step - loss: 0.5748 - accuracy: 0.8328\n",
      "Epoch 3/4\n",
      "4096/4096 [==============================] - 5s 1ms/step - loss: 0.2703 - accuracy: 0.9229\n",
      "Epoch 4/4\n",
      "4096/4096 [==============================] - 5s 1ms/step - loss: 0.1663 - accuracy: 0.9521\n",
      "Epoch 1/4\n",
      "4096/4096 [==============================] - 5s 1ms/step - loss: 1.5626 - accuracy: 0.6169\n",
      "Epoch 2/4\n",
      "4096/4096 [==============================] - 5s 1ms/step - loss: 0.5732 - accuracy: 0.8320\n",
      "Epoch 3/4\n",
      "4096/4096 [==============================] - 5s 1ms/step - loss: 0.2376 - accuracy: 0.9285\n",
      "Epoch 4/4\n",
      "4096/4096 [==============================] - 5s 1ms/step - loss: 0.1562 - accuracy: 0.9521\n",
      "Epoch 1/4\n",
      "1831/1831 [==============================] - 2s 1ms/step - loss: 1.3745 - accuracy: 0.6532\n",
      "Epoch 2/4\n",
      "1831/1831 [==============================] - 2s 1ms/step - loss: 0.5795 - accuracy: 0.8296\n",
      "Epoch 3/4\n",
      "1831/1831 [==============================] - 2s 1ms/step - loss: 0.2952 - accuracy: 0.9093\n",
      "Epoch 4/4\n",
      "1831/1831 [==============================] - 2s 1ms/step - loss: 0.1625 - accuracy: 0.9547\n",
      "Epoch 1/4\n",
      "4096/4096 [==============================] - 5s 1ms/step - loss: 1.7041 - accuracy: 0.5986\n",
      "Epoch 2/4\n",
      "4096/4096 [==============================] - 5s 1ms/step - loss: 0.6129 - accuracy: 0.8269\n",
      "Epoch 3/4\n",
      "4096/4096 [==============================] - 5s 1ms/step - loss: 0.2679 - accuracy: 0.9248\n",
      "Epoch 4/4\n",
      "4096/4096 [==============================] - 5s 1ms/step - loss: 0.1597 - accuracy: 0.9592\n",
      "Epoch 1/4\n",
      "4096/4096 [==============================] - 5s 1ms/step - loss: 1.5767 - accuracy: 0.6111\n",
      "Epoch 2/4\n",
      "4096/4096 [==============================] - 5s 1ms/step - loss: 0.5763 - accuracy: 0.8401\n",
      "Epoch 3/4\n",
      "4096/4096 [==============================] - 5s 1ms/step - loss: 0.2650 - accuracy: 0.9224\n",
      "Epoch 4/4\n",
      "4096/4096 [==============================] - 5s 1ms/step - loss: 0.1589 - accuracy: 0.9551\n",
      "Epoch 1/4\n",
      "4096/4096 [==============================] - 5s 1ms/step - loss: 1.5471 - accuracy: 0.6167\n",
      "Epoch 2/4\n",
      "4096/4096 [==============================] - 5s 1ms/step - loss: 0.5634 - accuracy: 0.8398\n",
      "Epoch 3/4\n",
      "4096/4096 [==============================] - 5s 1ms/step - loss: 0.2631 - accuracy: 0.9265\n",
      "Epoch 4/4\n",
      "4096/4096 [==============================] - 5s 1ms/step - loss: 0.1594 - accuracy: 0.9583\n",
      "Epoch 1/4\n",
      "4096/4096 [==============================] - 5s 1ms/step - loss: 1.5072 - accuracy: 0.6274\n",
      "Epoch 2/4\n",
      "4096/4096 [==============================] - 5s 1ms/step - loss: 0.5504 - accuracy: 0.8389\n",
      "Epoch 3/4\n",
      "4096/4096 [==============================] - 5s 1ms/step - loss: 0.2434 - accuracy: 0.9329\n",
      "Epoch 4/4\n",
      "4096/4096 [==============================] - 5s 1ms/step - loss: 0.1565 - accuracy: 0.9556\n",
      "Epoch 1/4\n",
      "4096/4096 [==============================] - 5s 1ms/step - loss: 1.5432 - accuracy: 0.6204\n",
      "Epoch 2/4\n",
      "4096/4096 [==============================] - 5s 1ms/step - loss: 0.5517 - accuracy: 0.8435\n",
      "Epoch 3/4\n",
      "4096/4096 [==============================] - 5s 1ms/step - loss: 0.2512 - accuracy: 0.9241\n",
      "Epoch 4/4\n",
      "4096/4096 [==============================] - 5s 1ms/step - loss: 0.1550 - accuracy: 0.9565\n",
      "Epoch 1/4\n",
      "4096/4096 [==============================] - 5s 1ms/step - loss: 1.6121 - accuracy: 0.6089\n",
      "Epoch 2/4\n",
      "4096/4096 [==============================] - 5s 1ms/step - loss: 0.5836 - accuracy: 0.8367\n",
      "Epoch 3/4\n",
      "4096/4096 [==============================] - 5s 1ms/step - loss: 0.2626 - accuracy: 0.9260\n",
      "Epoch 4/4\n",
      "4096/4096 [==============================] - 5s 1ms/step - loss: 0.1761 - accuracy: 0.9463\n",
      "Epoch 1/4\n",
      "4096/4096 [==============================] - 5s 1ms/step - loss: 1.5299 - accuracy: 0.6284\n",
      "Epoch 2/4\n",
      "4096/4096 [==============================] - 5s 1ms/step - loss: 0.5382 - accuracy: 0.8457\n",
      "Epoch 3/4\n",
      "4096/4096 [==============================] - 5s 1ms/step - loss: 0.2571 - accuracy: 0.9260\n",
      "Epoch 4/4\n",
      "4096/4096 [==============================] - 5s 1ms/step - loss: 0.1561 - accuracy: 0.9595\n",
      "Epoch 1/4\n",
      "4096/4096 [==============================] - 5s 1ms/step - loss: 1.4773 - accuracy: 0.6270\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4096/4096 [==============================] - 5s 1ms/step - loss: 0.5707 - accuracy: 0.8325\n",
      "Epoch 3/4\n",
      "4096/4096 [==============================] - 5s 1ms/step - loss: 0.2581 - accuracy: 0.9265\n",
      "Epoch 4/4\n",
      "4096/4096 [==============================] - 5s 1ms/step - loss: 0.1394 - accuracy: 0.9639\n",
      "Epoch 1/4\n",
      "4096/4096 [==============================] - 5s 1ms/step - loss: 1.5087 - accuracy: 0.6277\n",
      "Epoch 2/4\n",
      "4096/4096 [==============================] - 5s 1ms/step - loss: 0.5612 - accuracy: 0.8391\n",
      "Epoch 3/4\n",
      "4096/4096 [==============================] - 5s 1ms/step - loss: 0.2638 - accuracy: 0.9221\n",
      "Epoch 4/4\n",
      "4096/4096 [==============================] - 5s 1ms/step - loss: 0.1553 - accuracy: 0.9568\n",
      "Epoch 1/4\n",
      "4096/4096 [==============================] - 5s 1ms/step - loss: 1.5266 - accuracy: 0.6277\n",
      "Epoch 2/4\n",
      "4096/4096 [==============================] - 5s 1ms/step - loss: 0.5388 - accuracy: 0.8472\n",
      "Epoch 3/4\n",
      "4096/4096 [==============================] - 5s 1ms/step - loss: 0.2698 - accuracy: 0.9248\n",
      "Epoch 4/4\n",
      "4096/4096 [==============================] - 5s 1ms/step - loss: 0.1559 - accuracy: 0.9575\n",
      "Model: 1\n",
      "Epoch 1/4\n",
      "4096/4096 [==============================] - 1s 308us/step - loss: 2.9760 - accuracy: 0.3804\n",
      "Epoch 2/4\n",
      "4096/4096 [==============================] - 1s 311us/step - loss: 2.6221 - accuracy: 0.4382\n",
      "Epoch 3/4\n",
      "4096/4096 [==============================] - 1s 311us/step - loss: 2.3670 - accuracy: 0.4807\n",
      "Epoch 4/4\n",
      "4096/4096 [==============================] - 1s 311us/step - loss: 2.2166 - accuracy: 0.5117\n",
      "Epoch 1/4\n",
      "4096/4096 [==============================] - 1s 309us/step - loss: 3.0030 - accuracy: 0.3669\n",
      "Epoch 2/4\n",
      "4096/4096 [==============================] - 1s 313us/step - loss: 2.6431 - accuracy: 0.4355\n",
      "Epoch 3/4\n",
      "4096/4096 [==============================] - 1s 307us/step - loss: 2.3881 - accuracy: 0.4644\n",
      "Epoch 4/4\n",
      "4096/4096 [==============================] - 1s 310us/step - loss: 2.1942 - accuracy: 0.5051\n",
      "Epoch 1/4\n",
      "4096/4096 [==============================] - 1s 310us/step - loss: 3.0109 - accuracy: 0.3635\n",
      "Epoch 2/4\n",
      "4096/4096 [==============================] - 1s 314us/step - loss: 2.6337 - accuracy: 0.4243\n",
      "Epoch 3/4\n",
      "4096/4096 [==============================] - 1s 310us/step - loss: 2.3727 - accuracy: 0.4673\n",
      "Epoch 4/4\n",
      "4096/4096 [==============================] - 1s 310us/step - loss: 2.1790 - accuracy: 0.5073\n",
      "Epoch 1/4\n",
      "4096/4096 [==============================] - 1s 320us/step - loss: 2.9825 - accuracy: 0.3728\n",
      "Epoch 2/4\n",
      "4096/4096 [==============================] - 1s 311us/step - loss: 2.6408 - accuracy: 0.4343\n",
      "Epoch 3/4\n",
      "4096/4096 [==============================] - 1s 309us/step - loss: 2.3949 - accuracy: 0.4768\n",
      "Epoch 4/4\n",
      "4096/4096 [==============================] - 1s 314us/step - loss: 2.2057 - accuracy: 0.4998\n",
      "Epoch 1/4\n",
      "4096/4096 [==============================] - 1s 308us/step - loss: 2.8854 - accuracy: 0.3928\n",
      "Epoch 2/4\n",
      "4096/4096 [==============================] - 1s 310us/step - loss: 2.5650 - accuracy: 0.4539\n",
      "Epoch 3/4\n",
      "4096/4096 [==============================] - 1s 311us/step - loss: 2.3309 - accuracy: 0.4927\n",
      "Epoch 4/4\n",
      "4096/4096 [==============================] - 1s 309us/step - loss: 2.1496 - accuracy: 0.5293\n",
      "Epoch 1/4\n",
      "4096/4096 [==============================] - 1s 311us/step - loss: 2.9228 - accuracy: 0.3821\n",
      "Epoch 2/4\n",
      "4096/4096 [==============================] - 1s 314us/step - loss: 2.5810 - accuracy: 0.4387\n",
      "Epoch 3/4\n",
      "4096/4096 [==============================] - 1s 312us/step - loss: 2.3298 - accuracy: 0.4773\n",
      "Epoch 4/4\n",
      "4096/4096 [==============================] - 1s 313us/step - loss: 2.1479 - accuracy: 0.5139\n",
      "Epoch 1/4\n",
      "4096/4096 [==============================] - 1s 310us/step - loss: 2.9172 - accuracy: 0.3931\n",
      "Epoch 2/4\n",
      "4096/4096 [==============================] - 1s 310us/step - loss: 2.5378 - accuracy: 0.4475\n",
      "Epoch 3/4\n",
      "4096/4096 [==============================] - 1s 311us/step - loss: 2.3019 - accuracy: 0.4868\n",
      "Epoch 4/4\n",
      "4096/4096 [==============================] - 1s 310us/step - loss: 2.0993 - accuracy: 0.5261\n",
      "Epoch 1/4\n",
      "4096/4096 [==============================] - 1s 308us/step - loss: 2.8853 - accuracy: 0.3794\n",
      "Epoch 2/4\n",
      "4096/4096 [==============================] - 1s 309us/step - loss: 2.5080 - accuracy: 0.4519\n",
      "Epoch 3/4\n",
      "4096/4096 [==============================] - 1s 312us/step - loss: 2.2896 - accuracy: 0.4929\n",
      "Epoch 4/4\n",
      "4096/4096 [==============================] - 1s 309us/step - loss: 2.0801 - accuracy: 0.5208\n",
      "Epoch 1/4\n",
      "4096/4096 [==============================] - 1s 309us/step - loss: 2.8210 - accuracy: 0.3950\n",
      "Epoch 2/4\n",
      "4096/4096 [==============================] - 1s 312us/step - loss: 2.4564 - accuracy: 0.4583\n",
      "Epoch 3/4\n",
      "4096/4096 [==============================] - 1s 313us/step - loss: 2.2045 - accuracy: 0.4978\n",
      "Epoch 4/4\n",
      "4096/4096 [==============================] - 1s 310us/step - loss: 2.0227 - accuracy: 0.5334\n",
      "Epoch 1/4\n",
      "4096/4096 [==============================] - 1s 308us/step - loss: 2.7995 - accuracy: 0.3958\n",
      "Epoch 2/4\n",
      "4096/4096 [==============================] - 1s 311us/step - loss: 2.4396 - accuracy: 0.4595\n",
      "Epoch 3/4\n",
      "4096/4096 [==============================] - 1s 312us/step - loss: 2.2126 - accuracy: 0.5022\n",
      "Epoch 4/4\n",
      "4096/4096 [==============================] - 1s 312us/step - loss: 2.0240 - accuracy: 0.5369\n",
      "Epoch 1/4\n",
      "4096/4096 [==============================] - 1s 308us/step - loss: 2.7923 - accuracy: 0.4019\n",
      "Epoch 2/4\n",
      "4096/4096 [==============================] - 1s 311us/step - loss: 2.4070 - accuracy: 0.4678\n",
      "Epoch 3/4\n",
      "4096/4096 [==============================] - 1s 311us/step - loss: 2.2101 - accuracy: 0.4966\n",
      "Epoch 4/4\n",
      "4096/4096 [==============================] - 1s 310us/step - loss: 2.0039 - accuracy: 0.5325\n",
      "Epoch 1/4\n",
      "4096/4096 [==============================] - 1s 308us/step - loss: 2.8324 - accuracy: 0.3879\n",
      "Epoch 2/4\n",
      "4096/4096 [==============================] - 1s 309us/step - loss: 2.4542 - accuracy: 0.4609\n",
      "Epoch 3/4\n",
      "4096/4096 [==============================] - 1s 312us/step - loss: 2.2189 - accuracy: 0.4934\n",
      "Epoch 4/4\n",
      "4096/4096 [==============================] - 1s 311us/step - loss: 2.0489 - accuracy: 0.5281\n",
      "Epoch 1/4\n",
      "1831/1831 [==============================] - 1s 323us/step - loss: 2.7392 - accuracy: 0.4042\n",
      "Epoch 2/4\n",
      "1831/1831 [==============================] - 1s 323us/step - loss: 2.4668 - accuracy: 0.4615\n",
      "Epoch 3/4\n",
      "1831/1831 [==============================] - 1s 326us/step - loss: 2.2170 - accuracy: 0.4904\n",
      "Epoch 4/4\n",
      "1831/1831 [==============================] - 1s 320us/step - loss: 2.0197 - accuracy: 0.5429\n",
      "Epoch 1/4\n",
      "4096/4096 [==============================] - 1s 313us/step - loss: 2.8440 - accuracy: 0.3918\n",
      "Epoch 2/4\n",
      "4096/4096 [==============================] - 1s 311us/step - loss: 2.4537 - accuracy: 0.4636\n",
      "Epoch 3/4\n",
      "4096/4096 [==============================] - 1s 310us/step - loss: 2.1679 - accuracy: 0.5139\n",
      "Epoch 4/4\n",
      "4096/4096 [==============================] - 1s 309us/step - loss: 2.0027 - accuracy: 0.5498\n",
      "Epoch 1/4\n",
      "4096/4096 [==============================] - 1s 312us/step - loss: 2.7770 - accuracy: 0.3940\n",
      "Epoch 2/4\n",
      "4096/4096 [==============================] - 1s 312us/step - loss: 2.4281 - accuracy: 0.4614\n",
      "Epoch 3/4\n",
      "4096/4096 [==============================] - 1s 309us/step - loss: 2.2018 - accuracy: 0.5010\n",
      "Epoch 4/4\n",
      "4096/4096 [==============================] - 1s 311us/step - loss: 2.0024 - accuracy: 0.5342\n",
      "Epoch 1/4\n",
      "4096/4096 [==============================] - 1s 312us/step - loss: 2.8336 - accuracy: 0.3960\n",
      "Epoch 2/4\n",
      "4096/4096 [==============================] - 1s 312us/step - loss: 2.4612 - accuracy: 0.4578\n",
      "Epoch 3/4\n",
      "4096/4096 [==============================] - 1s 312us/step - loss: 2.1805 - accuracy: 0.5125\n",
      "Epoch 4/4\n",
      "4096/4096 [==============================] - 1s 311us/step - loss: 1.9984 - accuracy: 0.5347\n",
      "Epoch 1/4\n",
      "4096/4096 [==============================] - 1s 309us/step - loss: 2.7648 - accuracy: 0.3999\n",
      "Epoch 2/4\n",
      "4096/4096 [==============================] - 1s 312us/step - loss: 2.4373 - accuracy: 0.4688\n",
      "Epoch 3/4\n",
      "4096/4096 [==============================] - 1s 312us/step - loss: 2.2007 - accuracy: 0.5105\n",
      "Epoch 4/4\n",
      "4096/4096 [==============================] - 1s 312us/step - loss: 2.0218 - accuracy: 0.5396\n",
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4096/4096 [==============================] - 1s 312us/step - loss: 2.7309 - accuracy: 0.4119\n",
      "Epoch 2/4\n",
      "4096/4096 [==============================] - 1s 309us/step - loss: 2.3781 - accuracy: 0.4761\n",
      "Epoch 3/4\n",
      "4096/4096 [==============================] - 1s 317us/step - loss: 2.1321 - accuracy: 0.5205\n",
      "Epoch 4/4\n",
      "4096/4096 [==============================] - 1s 310us/step - loss: 1.9627 - accuracy: 0.5564\n",
      "Epoch 1/4\n",
      "4096/4096 [==============================] - 1s 308us/step - loss: 2.8409 - accuracy: 0.3958\n",
      "Epoch 2/4\n",
      "4096/4096 [==============================] - 1s 313us/step - loss: 2.4863 - accuracy: 0.4460\n",
      "Epoch 3/4\n",
      "4096/4096 [==============================] - 1s 310us/step - loss: 2.2324 - accuracy: 0.4946\n",
      "Epoch 4/4\n",
      "4096/4096 [==============================] - 1s 313us/step - loss: 2.0540 - accuracy: 0.5386\n",
      "Epoch 1/4\n",
      "4096/4096 [==============================] - 1s 312us/step - loss: 2.6816 - accuracy: 0.4199\n",
      "Epoch 2/4\n",
      "4096/4096 [==============================] - 1s 314us/step - loss: 2.3407 - accuracy: 0.4771\n",
      "Epoch 3/4\n",
      "4096/4096 [==============================] - 1s 309us/step - loss: 2.0987 - accuracy: 0.5232\n",
      "Epoch 4/4\n",
      "4096/4096 [==============================] - 1s 311us/step - loss: 1.8992 - accuracy: 0.5662\n",
      "Epoch 1/4\n",
      "4096/4096 [==============================] - 1s 316us/step - loss: 2.7321 - accuracy: 0.4238\n",
      "Epoch 2/4\n",
      "4096/4096 [==============================] - 1s 309us/step - loss: 2.3594 - accuracy: 0.4783\n",
      "Epoch 3/4\n",
      "4096/4096 [==============================] - 1s 311us/step - loss: 2.0963 - accuracy: 0.5305\n",
      "Epoch 4/4\n",
      "4096/4096 [==============================] - 1s 312us/step - loss: 1.9030 - accuracy: 0.5640\n",
      "Epoch 1/4\n",
      "4096/4096 [==============================] - 1s 317us/step - loss: 2.6120 - accuracy: 0.4463\n",
      "Epoch 2/4\n",
      "4096/4096 [==============================] - 1s 314us/step - loss: 2.3186 - accuracy: 0.4800\n",
      "Epoch 3/4\n",
      "4096/4096 [==============================] - 1s 315us/step - loss: 2.0485 - accuracy: 0.5281\n",
      "Epoch 4/4\n",
      "4096/4096 [==============================] - 1s 311us/step - loss: 1.8676 - accuracy: 0.5684\n",
      "Epoch 1/4\n",
      "4096/4096 [==============================] - 1s 313us/step - loss: 2.6786 - accuracy: 0.4148\n",
      "Epoch 2/4\n",
      "4096/4096 [==============================] - 1s 311us/step - loss: 2.3309 - accuracy: 0.4778\n",
      "Epoch 3/4\n",
      "4096/4096 [==============================] - 1s 311us/step - loss: 2.0688 - accuracy: 0.5291\n",
      "Epoch 4/4\n",
      "4096/4096 [==============================] - 1s 311us/step - loss: 1.8978 - accuracy: 0.5681\n"
     ]
    }
   ],
   "source": [
    "for (i, model) in enumerate(models):\n",
    "    print(\"Model:\", i)\n",
    "    for begin, end in batches:\n",
    "        x_batch, y_batch = load_batch(X_train, Y_train, begin, end)\n",
    "        model.fit(x_batch, y_batch, batch_size=256, epochs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (i, model) in enumerate(models):\n",
    "    model.save_weights(f\"weights_model{i+1}.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on development set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    preds = preds.reshape(preds.shape[1])\n",
    "    arr = np.asarray(preds).astype('float64')\n",
    "    log_preds_scaled = np.log(arr) / temperature\n",
    "    preds_scaled = np.exp(log_preds_scaled)\n",
    "    softmaxed = preds_scaled / np.sum(preds_scaled)\n",
    "    probas = np.random.multinomial(1, softmaxed, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = list(zip(X_dev, Y_dev))[:100]\n",
    "\n",
    "def evaluate_model(model, samples, temperature=1.4):\n",
    "    perplexity = 0\n",
    "    accuracy = 0\n",
    "    \n",
    "    for x, y in samples:\n",
    "        words_seq = encode_words(x, w2v).reshape(1, SEQUENCE_LENGTH, EMBEDDING_SIZE)\n",
    "        preds = model.predict(words_seq)[0]\n",
    "        likelihood = preds[word2idx[y]]\n",
    "        perplexity += np.log2(likelihood)\n",
    "\n",
    "        y_hat = idx2word[sample(model.predict(words_seq), temperature=temperature)]\n",
    "        if y_hat == y:\n",
    "            accuracy += 1\n",
    "\n",
    "    num_samples = len(samples)\n",
    "    perplexity = np.power(2, perplexity * -1/num_samples)\n",
    "    \n",
    "    return accuracy / num_samples, perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/.local/lib/python3.6/site-packages/ipykernel_launcher.py:4: RuntimeWarning: divide by zero encountered in log\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0: acc 39.0%, perp 50.01\n",
      "Model 1: acc 21.0%, perp 32.26\n"
     ]
    }
   ],
   "source": [
    "for (i, model) in enumerate(models):\n",
    "    acc, perp = evaluate_model(model, samples)\n",
    "    print(f\"Model {i}: acc {round(acc*100, 2)}%, perp {round(perp, 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "look at her face , it 's only way drum <newline> i 'll have a little call me to show you <pad> , me knows a perfect day <newline> slipping here you that everybody me <newline> but you know what lovers something lonely <newline> just sing short it between baby say too lookin ( hehehe ) <newline> but there what who but yet ever dare can\n"
     ]
    }
   ],
   "source": [
    "#words = [\"never\", \"gonna\", \"give\", \"you\", \"up\", \",\"]\n",
    "words = [\"look\", \"at\", \"her\", \"face\", \",\", \"it\"]\n",
    "#words = [\"when\", \"there\", \"'s\", \"a\", \"dark\", \"storm\"]\n",
    "#words = [\"do\", \"better\", \",\", \"who\", \"better\", \"?\"]\n",
    "words_seq = encode_words(words, w2v)\n",
    "words_seq = words_seq.reshape(1, SEQUENCE_LENGTH, EMBEDDING_SIZE)\n",
    "#print(' '.join([decode_vec(words[0][i], w2v) for i in range(SEQUENCE_LENGTH)]))\n",
    "\n",
    "result = words\n",
    "for j in range(60):\n",
    "    word = idx2word[sample(model2.predict(words_seq), temperature=1.4)]\n",
    "    #word = one_hot_decode(model.predict(words_seq), idx2word)\n",
    "    result.append(word)\n",
    "    \n",
    "    new_words = np.zeros((1, SEQUENCE_LENGTH, EMBEDDING_SIZE))\n",
    "    for i in range(SEQUENCE_LENGTH-1):\n",
    "        new_words[0, i] = words_seq[0, i+1]\n",
    "    new_words[0, SEQUENCE_LENGTH-1] = encode_word(word, w2v)\n",
    "    words_seq = new_words\n",
    "\n",
    "#print(' '.join([decode_vec(words[0][i], w2v) for i in range(SEQUENCE_LENGTH)]))\n",
    "\n",
    "print(' '.join(result))\n",
    "    #words = new_words\n",
    "        #new_words[0] = words[0, 1]\n",
    "        #new_words[0, 1] = words[0, 2]\n",
    "        #new_words[0, 2] = words[0, 3]\n",
    "        #new_words[0, 3] = encode_word(word, word2idx)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
