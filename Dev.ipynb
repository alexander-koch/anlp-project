{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "w2v = KeyedVectors.load_word2vec_format('glove.6B.100d.txt.word2vec', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "SEQUENCE_LENGTH = 6\n",
    "HIDDEN_SIZE = 256\n",
    "\n",
    "EMBEDDING_SIZE_ORIG = 100\n",
    "EMBEDDING_SIZE = 103\n",
    "\n",
    "def encode_word(word, w2v):\n",
    "    if word == \"<pad>\":\n",
    "        v = np.zeros((EMBEDDING_SIZE,))\n",
    "        v[EMBEDDING_SIZE-1] = 1\n",
    "        return v\n",
    "    elif word == \"<newline>\":\n",
    "        v = np.zeros((EMBEDDING_SIZE,))\n",
    "        v[EMBEDDING_SIZE-2] = 1\n",
    "        return v\n",
    "    elif word == \"<unk>\" or word not in w2v:\n",
    "        v = np.zeros((EMBEDDING_SIZE,))\n",
    "        v[EMBEDDING_SIZE-3] = 1\n",
    "        return v\n",
    "    else:        \n",
    "        v = w2v[word]\n",
    "        w = np.zeros((3,))\n",
    "        return np.append(v, w, axis=0)\n",
    "\n",
    "def encode_words(words, w2v):\n",
    "    vec = np.zeros((len(words), EMBEDDING_SIZE))\n",
    "    for (i,word) in enumerate(words):\n",
    "        vec[i] = encode_word(word, w2v)\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(103,)\n",
      "(103,)\n"
     ]
    }
   ],
   "source": [
    "v = encode_word(\"hello\", w2v)\n",
    "print(v.shape)\n",
    "w = encode_word(\"boom-a-boomerang\", w2v)\n",
    "print(w.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101\n",
      "102\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "v_newline = encode_word(\"<newline>\", w2v)\n",
    "v_pad = encode_word(\"<pad>\", w2v)\n",
    "v_unk = encode_word(\"<unk>\", w2v)\n",
    "\n",
    "print(np.argmax(v_newline))\n",
    "print(np.argmax(v_pad))\n",
    "print(np.argmax(v_unk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_vec(vec, w2v):\n",
    "    base_vec = vec[:EMBEDDING_SIZE_ORIG]\n",
    "    ext_vec = vec[EMBEDDING_SIZE_ORIG:]\n",
    "    if ext_vec[0]:\n",
    "        return \"<unk>\"\n",
    "    elif ext_vec[1]:\n",
    "        return \"<newline>\"\n",
    "    elif ext_vec[2]:\n",
    "        return \"<pad>\"\n",
    "    else:\n",
    "        return w2v.similar_by_vector(base_vec)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      "<newline>\n",
      "<pad>\n",
      "<unk>\n"
     ]
    }
   ],
   "source": [
    "print(decode_vec(v, w2v))\n",
    "print(decode_vec(v_newline, w2v))\n",
    "print(decode_vec(v_pad, w2v))\n",
    "print(decode_vec(v_unk, w2v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_song(song, buffer_length):\n",
    "    tokens = song\n",
    "\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    for i in range(0, len(song)):\n",
    "        if i+buffer_length+1 >= len(tokens):\n",
    "            pad_length = (i+buffer_length+1) - len(tokens)\n",
    "            tokens += ['<pad>'] * pad_length\n",
    "\n",
    "        x_train.append(tokens[i:i+buffer_length])\n",
    "        y_train.append(tokens[i+buffer_length])\n",
    "\n",
    "    return x_train,y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_vocab = {'<pad>', '<unk>'}\n",
    "songs = []\n",
    "with open(\"data/sentences.txt\", \"r\") as f:\n",
    "    for line in f.readlines():\n",
    "        tokens = [token for token in line.rstrip().split(\" \")]\n",
    "        songs.append(tokens)\n",
    "        token_vocab = token_vocab.union(set(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8836\n",
      "['look', 'at', 'her', 'face', ',', 'it']\n"
     ]
    }
   ],
   "source": [
    "x_vec = []\n",
    "y_vec = []\n",
    "for song in songs[:30]:\n",
    "    x_vec_i, y_vec_i = prepare_song(song, SEQUENCE_LENGTH)\n",
    "    x_vec.extend(x_vec_i)\n",
    "    y_vec.extend(y_vec_i)\n",
    "print(len(x_vec))\n",
    "print(x_vec[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 5111\n",
      "W2V vocab size: 400000\n"
     ]
    }
   ],
   "source": [
    "words = list(token_vocab)\n",
    "vocab_size = len(words)\n",
    "print(\"Vocab size:\", vocab_size)\n",
    "print(\"W2V vocab size:\", len(w2v.vocab))\n",
    "word2idx = { word:i for i,word in enumerate(words) }\n",
    "idx2word = { i:word for i,word in enumerate(words) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(word, word2idx):\n",
    "    v = np.zeros((len(word2idx, )))\n",
    "    v[word2idx[word]] = 1\n",
    "    return v\n",
    "\n",
    "def one_hot_decode(word, idx2word):\n",
    "    return idx2word[np.argmax(word)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5028\n",
      "5028\n"
     ]
    }
   ],
   "source": [
    "print(word2idx[\"hello\"])\n",
    "print(np.argmax(one_hot_encode(\"hello\", word2idx)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "def generate_batches(data_length, mini_batch_size):\n",
    "    for begin in range(0, data_length, mini_batch_size):\n",
    "        end = min(begin + mini_batch_size, data_length)\n",
    "        yield begin, end\n",
    "\n",
    "def load_batch(xs, ys, begin, end):\n",
    "    batch_size = end-begin\n",
    "    \n",
    "    x_train = np.zeros((batch_size, SEQUENCE_LENGTH, EMBEDDING_SIZE))\n",
    "    y_train = np.zeros((batch_size, vocab_size))\n",
    "    \n",
    "    xs_batch = xs[begin:end]\n",
    "    ys_batch = ys[begin:end]\n",
    "    \n",
    "    c = list(zip(xs_batch, ys_batch))\n",
    "    shuffle(c)\n",
    "    xs_batch, ys_batch = zip(*c)\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        x_train[i] = encode_words(xs_batch[i], w2v)\n",
    "        y_train[i] = one_hot_encode(ys_batch[i], word2idx)\n",
    "    \n",
    "    return x_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 6, 103) (512, 5111)\n",
      "X train\n",
      "i am your music and i\n",
      "<newline> please do n't talk ,\n",
      "? <newline> she 's just my\n",
      "could ever believe that she could\n",
      "who could ever believe that she\n",
      "walking for hours and talking <newline>\n",
      "to me <newline> look at the\n",
      "down <newline> there 's a shimmer\n",
      "about all the things that we\n",
      "and make me strong <newline> (\n",
      "\n",
      "Y train\n",
      "am\n",
      "go\n",
      "kind\n",
      "be\n",
      "could\n",
      "about\n",
      "way\n",
      "in\n",
      "plan\n",
      "play\n"
     ]
    }
   ],
   "source": [
    "batches = generate_batches(len(x_vec), 512)\n",
    "begin, end = next(batches)\n",
    "\n",
    "x_train, y_train = load_batch(x_vec, y_vec, begin, end)\n",
    "print(x_train.shape, y_train.shape)\n",
    "\n",
    "print(\"X train\")\n",
    "for j in range(10):\n",
    "    ws = ' '.join([decode_vec(x_train[j][i], w2v) for i in range(SEQUENCE_LENGTH)])\n",
    "    print(ws)\n",
    "    \n",
    "print(\"\\nY train\")\n",
    "for j in range(10):\n",
    "    print(one_hot_decode(y_train[j], idx2word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, LSTM, Embedding, Dropout\n",
    "from keras.layers import LeakyReLU\n",
    "def build_model(vocab_size):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(128, input_shape=(SEQUENCE_LENGTH, EMBEDDING_SIZE), return_sequences=True))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(LSTM(1024))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(2048))\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    model.add(Dense(vocab_size, activation='softmax'))\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer=\"rmsprop\", metrics = ['accuracy'])\n",
    "    return model\n",
    "model = build_model(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = list(generate_batches(len(x_vec), 2048))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "644/644 [==============================] - 1s 1ms/step - loss: 8.2761 - accuracy: 0.1211\n",
      "Epoch 2/10\n",
      "644/644 [==============================] - 1s 1ms/step - loss: 4.2993 - accuracy: 0.3090\n",
      "Epoch 3/10\n",
      "644/644 [==============================] - 1s 1ms/step - loss: 2.4572 - accuracy: 0.5264\n",
      "Epoch 4/10\n",
      "644/644 [==============================] - 1s 1ms/step - loss: 1.5718 - accuracy: 0.6832\n",
      "Epoch 5/10\n",
      "644/644 [==============================] - 1s 1ms/step - loss: 0.9197 - accuracy: 0.7966\n",
      "Epoch 6/10\n",
      "644/644 [==============================] - 1s 1ms/step - loss: 0.6501 - accuracy: 0.8525\n",
      "Epoch 7/10\n",
      "644/644 [==============================] - 1s 1ms/step - loss: 0.4178 - accuracy: 0.9130\n",
      "Epoch 8/10\n",
      "644/644 [==============================] - 1s 1ms/step - loss: 0.2877 - accuracy: 0.9363\n",
      "Epoch 9/10\n",
      "644/644 [==============================] - 1s 1ms/step - loss: 0.2622 - accuracy: 0.9394\n",
      "Epoch 10/10\n",
      "644/644 [==============================] - 1s 1ms/step - loss: 0.1846 - accuracy: 0.9596\n",
      "Epoch 1/10\n",
      "2048/2048 [==============================] - 2s 1ms/step - loss: 5.6628 - accuracy: 0.2280\n",
      "Epoch 2/10\n",
      "2048/2048 [==============================] - 2s 992us/step - loss: 2.9918 - accuracy: 0.4297\n",
      "Epoch 3/10\n",
      "2048/2048 [==============================] - 2s 1ms/step - loss: 2.0078 - accuracy: 0.5669\n",
      "Epoch 4/10\n",
      "2048/2048 [==============================] - 2s 1ms/step - loss: 1.2073 - accuracy: 0.7246\n",
      "Epoch 5/10\n",
      "2048/2048 [==============================] - 2s 1ms/step - loss: 0.7653 - accuracy: 0.8174\n",
      "Epoch 6/10\n",
      "2048/2048 [==============================] - 2s 999us/step - loss: 0.5276 - accuracy: 0.8672\n",
      "Epoch 7/10\n",
      "2048/2048 [==============================] - 2s 999us/step - loss: 0.3509 - accuracy: 0.9175\n",
      "Epoch 8/10\n",
      "2048/2048 [==============================] - 2s 1ms/step - loss: 0.2869 - accuracy: 0.9302\n",
      "Epoch 9/10\n",
      "2048/2048 [==============================] - 2s 1ms/step - loss: 0.2331 - accuracy: 0.9326\n",
      "Epoch 10/10\n",
      "2048/2048 [==============================] - 2s 1ms/step - loss: 0.1957 - accuracy: 0.9526\n",
      "Epoch 1/10\n",
      "2048/2048 [==============================] - 2s 1ms/step - loss: 4.2457 - accuracy: 0.3296\n",
      "Epoch 2/10\n",
      "2048/2048 [==============================] - 2s 1ms/step - loss: 2.0890 - accuracy: 0.5840\n",
      "Epoch 3/10\n",
      "2048/2048 [==============================] - 2s 1ms/step - loss: 1.3364 - accuracy: 0.6992\n",
      "Epoch 4/10\n",
      "2048/2048 [==============================] - 2s 1ms/step - loss: 0.9022 - accuracy: 0.7744\n",
      "Epoch 5/10\n",
      "2048/2048 [==============================] - 2s 1ms/step - loss: 0.5977 - accuracy: 0.8374\n",
      "Epoch 6/10\n",
      "2048/2048 [==============================] - 2s 1ms/step - loss: 0.4262 - accuracy: 0.8882\n",
      "Epoch 7/10\n",
      "2048/2048 [==============================] - 2s 1ms/step - loss: 0.3026 - accuracy: 0.9219\n",
      "Epoch 8/10\n",
      "2048/2048 [==============================] - 2s 1ms/step - loss: 0.2475 - accuracy: 0.9360\n",
      "Epoch 9/10\n",
      "2048/2048 [==============================] - 2s 1ms/step - loss: 0.1833 - accuracy: 0.9565\n",
      "Epoch 10/10\n",
      "2048/2048 [==============================] - 2s 1ms/step - loss: 0.2036 - accuracy: 0.9443\n",
      "Epoch 1/10\n",
      "2048/2048 [==============================] - 2s 1ms/step - loss: 4.8178 - accuracy: 0.3120\n",
      "Epoch 2/10\n",
      "2048/2048 [==============================] - 2s 1ms/step - loss: 1.9271 - accuracy: 0.6001\n",
      "Epoch 3/10\n",
      "2048/2048 [==============================] - 2s 1ms/step - loss: 1.1193 - accuracy: 0.7432\n",
      "Epoch 4/10\n",
      "2048/2048 [==============================] - 2s 1ms/step - loss: 0.6851 - accuracy: 0.8286\n",
      "Epoch 5/10\n",
      "2048/2048 [==============================] - 2s 1ms/step - loss: 0.4403 - accuracy: 0.8936\n",
      "Epoch 6/10\n",
      "2048/2048 [==============================] - 2s 1ms/step - loss: 0.3135 - accuracy: 0.9165\n",
      "Epoch 7/10\n",
      "2048/2048 [==============================] - 2s 1ms/step - loss: 0.2527 - accuracy: 0.9346\n",
      "Epoch 8/10\n",
      "2048/2048 [==============================] - 2s 1ms/step - loss: 0.2149 - accuracy: 0.9424\n",
      "Epoch 9/10\n",
      "2048/2048 [==============================] - 2s 1ms/step - loss: 0.2050 - accuracy: 0.9541\n",
      "Epoch 10/10\n",
      "2048/2048 [==============================] - 2s 1ms/step - loss: 0.1701 - accuracy: 0.9531\n"
     ]
    }
   ],
   "source": [
    "shuffle(batches)\n",
    "for i in range(4):\n",
    "    begin, end = batches[i]\n",
    "    x_train, y_train = load_batch(x_vec, y_vec, begin, end)\n",
    "    model.fit(x_train, y_train, batch_size=256, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    preds = preds.reshape(preds.shape[1])\n",
    "    arr = np.asarray(preds).astype('float64')\n",
    "    log_preds_scaled = np.log(arr) / temperature\n",
    "    preds_scaled = np.exp(log_preds_scaled)\n",
    "    softmaxed = preds_scaled / np.sum(preds_scaled)\n",
    "    probas = np.random.multinomial(1, softmaxed, 1)\n",
    "    return np.argmax(probas)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "look at her face , it 's a party <newline> i remember sitting half <newline> little while half in the clay <newline> i 'm still trapped my , <newline> cold gim yet there another end without <newline> there <newline> fixin angel , going <newline> loving it do <newline> please knew empty feeling what again hairs in the dream <newline> when the twilight seems like through to\n"
     ]
    }
   ],
   "source": [
    "#words = [\"never\", \"gonna\", \"give\", \"you\", \"up\", \",\"]\n",
    "words = [\"look\", \"at\", \"her\", \"face\", \",\", \"it\"]\n",
    "#words = [\"when\", \"there\", \"'s\", \"a\", \"dark\", \"storm\"]\n",
    "#words = [\"do\", \"better\", \",\", \"who\", \"better\", \"?\"]\n",
    "words_seq = encode_words(words, w2v)\n",
    "words_seq = words_seq.reshape(1, SEQUENCE_LENGTH, EMBEDDING_SIZE)\n",
    "#print(' '.join([decode_vec(words[0][i], w2v) for i in range(SEQUENCE_LENGTH)]))\n",
    "\n",
    "result = words\n",
    "for j in range(60):\n",
    "    word = idx2word[sample(model.predict(words_seq), temperature=1.4)]\n",
    "    #word = one_hot_decode(model.predict(words_seq), idx2word)\n",
    "    result.append(word)\n",
    "    \n",
    "    new_words = np.zeros((1, SEQUENCE_LENGTH, EMBEDDING_SIZE))\n",
    "    for i in range(SEQUENCE_LENGTH-1):\n",
    "        new_words[0, i] = words_seq[0, i+1]\n",
    "    new_words[0, SEQUENCE_LENGTH-1] = encode_word(word, w2v)\n",
    "    words_seq = new_words\n",
    "\n",
    "#print(' '.join([decode_vec(words[0][i], w2v) for i in range(SEQUENCE_LENGTH)]))\n",
    "\n",
    "print(' '.join(result))\n",
    "    #words = new_words\n",
    "        #new_words[0] = words[0, 1]\n",
    "        #new_words[0, 1] = words[0, 2]\n",
    "        #new_words[0, 2] = words[0, 3]\n",
    "        #new_words[0, 3] = encode_word(word, word2idx)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
